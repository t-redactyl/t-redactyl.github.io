<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Two-group hypothesis testing: independent samples t-tests</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Two-group hypothesis testing: independent samples t-tests written September 30, 2015 in statistics,r,data simulations,hypothesis testing">
  <meta name="keywords" content="rlanguage, t test, simulations, data science, hypothesis testing, a/b testing" />

    <link
      rel="canonical"
      href="http://t-redactyl.github.io/blog/2015/09/two-group-hypothesis-testing-independent-samples-t-tests.html"
    />

    <link href="http://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="http://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="http://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="http://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="http://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Two-group hypothesis testing: independent samples t-tests</h1>
        <div class="meta">
          written <time datetime="2015-09-30T00:00:00+02:00">September 30, 2015</time>
          in <span class="categories">
            <a href="http://t-redactyl.github.io/tag/statistics.html">statistics</a>,            <a href="http://t-redactyl.github.io/tag/r.html">r</a>,            <a href="http://t-redactyl.github.io/tag/data-simulations.html">data simulations</a>,            <a href="http://t-redactyl.github.io/tag/hypothesis-testing.html">hypothesis testing</a>          </span>
        </div>
        <p>In some of my previous posts, I asked you to imagine that we work for a retail website that sells children&#8217;s toys. In the past, they&#8217;ve asked us to estimate the mean number of page views per day (see <a href="({filename}2015-09-01-A-Gentle-Introduction-to-the-Standard-Error-of-the-Mean.md)">here</a> and <a href="({filename}2015-09-08-a-gentle-introduction-to-bootstrapping.md)">here</a> for my posts discussing this problem). Now, they&#8217;ve launched two online advertising campaigns, and they want us to see if these campaigns are equally effective or one is better than the other (a technique known as <a href="https://en.wikipedia.org/wiki/A/B_testing">A/B testing</a> or <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">randomised trials</a>).</p>
<p>The way we assess this is a central method in statistical inference called <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"><strong>hypothesis testing</strong></a>. The usual workflow of hypothesis testing is as follows:<br>
1. Define your question; <br>
2. Define your hypotheses;   <br>
3. Pick the most likely appropriate test distribution (t, Z, binomial, etc.);<br>
4. Compute your test statistic;<br>
5. Work out whether we reject the null hypothesis based whether your test statistic exceeds a critical value under this&nbsp;distribution.</p>
<p>This blog post will through each of these steps in detail, using our advertising campaign problem as an&nbsp;example.</p>
<h2>Defining your&nbsp;question</h2>
<p>The first, and most important step to any analysis is working out <strong>what you are asking</strong> and <strong>how you will measure it</strong>. A reasonable way to assess whether the advertising campaigns are equally effective or not could be to take all site visits that originate from each of the campaigns and see how much money the company makes from these visits (i.e., the amount of toys the customers who visit buy). A way we could then test if the amount generated differs is to take the mean amount of money made from each campaign and statistically test whether these means are&nbsp;different.</p>
<h2>Defining your&nbsp;hypotheses</h2>
<p>The next step is defining hypotheses so we can test these questions statistically. When you define hypotheses, you are trying to compare two possible outcomes. The first is the <a href="https://en.wikipedia.org/wiki/Null_hypothesis"><strong>null hypothesis</strong></a> (<span class="math">\(H_0\)</span>), which represents the &#8220;status quo&#8221; and is assumed to be correct until statistical evidence is presented that allows us to reject it. In this case, the null hypothesis is that there is no difference between the mean amount of income generated by each campaign. If we assign <span class="math">\(\mu_1\)</span> to be the mean of the first population, and <span class="math">\(\mu_2\)</span> to be the mean of the second population, these hypotheses can be stated&nbsp;as:</p>
<div class="math">$$H_0 : \mu_1 = \mu_2$$</div>
<p><center>or</center></p>
<div class="math">$$H_0 : \mu_1 - \mu_2 = 0$$</div>
<p>The <a href="https://en.wikipedia.org/wiki/Alternative_hypothesis"><strong>alternative hypothesis</strong></a> (<span class="math">\(H_a\)</span>) is that there is a difference between the mean level of income generated by each campaign. More formally, the alternative hypothesis&nbsp;is:</p>
<div class="math">$$H_a : \mu_1 \neq \mu_2$$</div>
<p><center>or</center></p>
<div class="math">$$H_a : \mu_1 - \mu_2 \neq 0$$</div>
<p>In other words, we are trying to test whether the difference in the mean levels of income generated by each campaign is sufficiently different from 0 to be&nbsp;meaningful.</p>
<h2>Picking the most appropriate&nbsp;distribution</h2>
<p>The most appropriate distribution for our test depends on what we <em>assume the population distribution is</em>. As the next step in our study of which campaign is correct, we take <a href="({filename}2015-09-15-representative-sampling.md)"><strong>representative samples</strong></a> of site visits originating from each campaign and record how much was purchased (simulated&nbsp;below):</p>
<div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">567</span><span class="p">)</span>
<span class="n">campaign.1</span> <span class="o">&lt;-</span> <span class="nf">rt</span><span class="p">(</span><span class="m">40</span><span class="p">,</span> <span class="m">39</span><span class="p">)</span> <span class="o">*</span> <span class="m">60</span> <span class="o">+</span> <span class="m">310</span>
<span class="n">campaign.2</span> <span class="o">&lt;-</span> <span class="nf">rt</span><span class="p">(</span><span class="m">40</span><span class="p">,</span> <span class="m">39</span><span class="p">)</span> <span class="o">*</span> <span class="m">58</span> <span class="o">+</span> <span class="m">270</span>
</pre></div>


<p><img alt="plot of chunk advertising_sample_plots" src="/figure/advertising_sample_plots-1.png"> </p>
<p>When we look at the data, it appears close enough to normal. However, our sample is a bit small (40 per campaign), so we should be a bit cautious about using the normal (Z) distribution. Instead, we&#8217;ll use a t-distribution, which performs better with &#8220;normally-shaped&#8221; data that have small sample sizes. According to our samples, the first advertising campaign generated a mean of \<span class="math">\(296.42 per visit with a standard deviation of \\)</span>65.9, and the second campaign generated a mean of \<span class="math">\(267.11 per visit with a standard deviation of \\)</span>43.53.</p>
<p>(<strong>Technical aside:</strong> The reason that the t-distribution performs better than the normal distribution with small samples is because we use the sample standard deviation in our calculation of both the t- and Z-distributions, rather than the true population standard deviation. At large samples, the sample standard deviation is expected to be a very close approximation to the population standard deviation; however, this is not the case in smaller samples. As such, using a Z-distribution for small samples leads to an underestimation of the standard error of the mean and consequently, confidence intervals. Incidently, this also means that as you collect more and more data, the t-distribution behaves more and more like the Z-distribution, meaning that it is a safe bet to use the t-distribution if you are not sure if your sample is big&nbsp;&#8220;enough&#8221;.)</p>
<h2>Computing your test&nbsp;statistic</h2>
<p>The next step is to get some measure of whether these values are different (the <a href="https://en.wikipedia.org/wiki/Test_statistic"><strong>test statistic</strong></a>). When talking about hypothesis tests, I pointed out that the null hypothesis can be reframed as <span class="math">\(\mu_1 - \mu_2 = 0\)</span>, and the alternative hypothesis as <span class="math">\(\mu_1 - \mu_2 \neq 0\)</span>. As such, we can test our hypotheses by taking the difference of our two campaigns as the <strong>difference between the two means</strong>.</p>
<div class="highlight"><pre><span></span><span class="n">diff.means</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">campaign.1</span><span class="p">)</span> <span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">campaign.2</span><span class="p">)</span>
</pre></div>


<p>This gives us a difference between the campaigns of \<span class="math">\(29.31 per visit. In order to test whether this difference is statistically meaningful, we need to convert it into the same units as our test distribution (more on this below) by standardising it. To standardise the mean difference, we subtract the value of the mean difference under the null hypothesis (which is 0) from $\mu_1 - \mu_2\)</span>, and then divide it by the pooled standard error. This standardised mean difference is the <a href="https://en.wikipedia.org/wiki/T-statistic"><strong>t-value.</strong></a> (Of course, we don&#8217;t have the true population value of the means and standard deviations, so we use the sample estimates&nbsp;instead.)</p>
<div class="highlight"><pre><span></span><span class="c1"># First calculate the pooled standard deviation (assuming equal variances, with equal sample sizes)</span>
<span class="n">sp</span> <span class="o">&lt;-</span> <span class="nf">sqrt</span><span class="p">((</span><span class="nf">sd</span><span class="p">(</span><span class="n">campaign.1</span><span class="p">)</span><span class="n">^2</span> <span class="o">+</span> <span class="nf">sd</span><span class="p">(</span><span class="n">campaign.2</span><span class="p">)</span><span class="n">^2</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">)</span>

<span class="c1"># Then calculate the standard error of the mean difference</span>
<span class="n">se</span> <span class="o">&lt;-</span> <span class="n">sp</span> <span class="o">*</span> <span class="p">(</span><span class="m">1</span> <span class="o">/</span> <span class="nf">length</span><span class="p">(</span><span class="n">campaign.1</span><span class="p">)</span> <span class="o">+</span> <span class="m">1</span> <span class="o">/</span> <span class="nf">length</span><span class="p">(</span><span class="n">campaign.1</span><span class="p">))</span><span class="n">^.5</span>

<span class="c1"># The t-value is the difference in means divided by the standard error</span>
<span class="n">t.value</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">diff.means</span> <span class="o">-</span> <span class="m">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">se</span>
</pre></div>


<p>The t-value in this case is 2.347. We will work out if this is meaningful in the next&nbsp;step.</p>
<h2>Rejecting or accepting the null&nbsp;hypothesis</h2>
<p>The final step is working out whether our means are &#8220;different enough&#8221; is to define a <strong>rejection region</strong> around our mean of 0 (no difference). If our t-value falls within the rejection region, we can call our results statistically&nbsp;significant.</p>
<p>In order to define our rejection region, we need to first decide on our <a href="https://en.wikipedia.org/wiki/Statistical_significance"><strong>significance level</strong></a>. The significance level represents the probability of <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors"><strong>Type I error</strong></a> we are willing to tolerate in our study, or the probability that we reject the null hypothesis when it is actually true. The &#8220;benchmark&#8221; significance level is 0.05, or a 5% chance that we incorrectly reject the null hypothesis (in our case, saying the means are different when they are in fact&nbsp;not).</p>
<p>Once we have decided on our significance level, we need to find the <strong>critical values</strong> that represent the point beyond which 5% of our test distribution lies. The specific critical value depends on both the shape of the distribution and whether you are using a <a href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests"><strong>one-sided</strong> versus a <strong>two-sided</strong> test</a>. One-sided tests are appropriate when you are trying to assess whether one value is bigger (i.e., <span class="math">\(\mu_1 - \mu_2 \gt 0\)</span>) or smaller than another (i.e., <span class="math">\(\mu_1 - \mu_2 \lt 0\)</span>). For example, if we were trying to test whether campaign 1 brought in more revenue than campaign 2, it would be appropriate for us to use a one-sided test. In this case, we would have the entire 5% rejection region in the right-hand tail of the distribution, as in the figure&nbsp;below.</p>
<p>However, as we are looking for any difference (whether positive or negative) between the two means, we must use a two sided test. In a two-sided test, the rejection region is split in half, so that 2.5% lies in the left-hand tail and 2.5% lies in the right-hand tail of the distribution. As you can see in the figure below, the implication of this is that we need less extreme test statistics to achieve significance in a one-sided test than in a two-sided, therefore it is very bad practice to incorrectly use a one-sided test when a two-sided test would be&nbsp;appropriate.</p>
<p><img alt="plot of chunk rejection_region_plots" src="/figure/rejection_region_plots-1.png"> </p>
<h3>Just eat the damn&nbsp;orange!</h3>
<p>Ok, after all of that, we can see whether our t-value lies in the rejection region. We generate the specific two-sided critical values for our distribution, and then compare this to our&nbsp;t-value.</p>
<div class="highlight"><pre><span></span><span class="c1"># Generate the 95% confidence interval.</span>
<span class="n">lci</span> <span class="o">&lt;-</span> <span class="m">-1</span> <span class="o">*</span> <span class="nf">qt</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">.975</span><span class="p">),</span> <span class="m">78</span><span class="p">)</span>
<span class="n">uci</span> <span class="o">&lt;-</span> <span class="nf">qt</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">.975</span><span class="p">),</span> <span class="m">78</span><span class="p">)</span>
</pre></div>


<p><img alt="plot of chunk t_test_plot" src="/figure/t_test_plot-1.png"> </p>
<p>The rejection region lies below -1.991 and above 1.991. The t-value is 2.347, therefore we can say the difference in the outcomes generated between the two campaigns is <strong>significant at the 0.05 level</strong>.</p>
<h2>Let&#8217;s repeat in&nbsp;R!</h2>
<p>Now I&#8217;ve shown you how a t-test works under the hood, let&#8217;s go through how you would <strong>actually</strong> do this in your day-to-day use of&nbsp;R.</p>
<p>All we need to do is call the t.test function, like&nbsp;so:</p>
<div class="highlight"><pre><span></span><span class="n">test</span> <span class="o">&lt;-</span> <span class="nf">t.test</span><span class="p">(</span><span class="n">campaign.1</span><span class="p">,</span> <span class="n">campaign.2</span><span class="p">,</span> <span class="n">paired</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span> <span class="n">var.equal</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
<span class="n">test</span> 
</pre></div>


<div class="highlight"><pre><span></span><span class="err">## </span>
<span class="err">##  Two Sample t-test</span>
<span class="err">## </span>
<span class="err">## data:  campaign.1 and campaign.2</span>
<span class="err">## t = 2.3473, df = 78, p-value = 0.02145</span>
<span class="err">## alternative hypothesis: true difference in means is not equal to 0</span>
<span class="err">## 95 percent confidence interval:</span>
<span class="err">##   4.450556 54.169379</span>
<span class="err">## sample estimates:</span>
<span class="err">## mean of x mean of y </span>
<span class="err">##  296.4194  267.1094</span>
</pre></div>


<p>As you can see, the results are the same. The t-value (or test statistic) is 2.347 and our p-value is 0.021, which is less than&nbsp;0.05.</p>
<h2>Statistical versus practical&nbsp;significance</h2>
<p>In the end, we found out that the two advertising campaigns generated significantly different amounts of revenue per visit, with campaign 1 generating a mean of \$29.31 more per visit than campaign 2. However, does this automatically mean we go with campaign 1? For example, what if the reason that campaign 1 is generating more income is because the ads are being placed on high-end websites, where the sort of people seeing the ad have more disposible income than the general population, but the ad costs are much more expensive? Is the revenue difference big enough to justify the increased cost of advertising on this&nbsp;platform?</p>
<h2>Take away&nbsp;message</h2>
<p>In this post I talked you through a core technique in statistical inference, the two-sample t-test. While this is a very straightforward test to apply in R, choosing when it is an appropriate test to use and whether your data and hypotheses meet the assumptions of this test can be less clear. In addition, the results of a significant test must be interpreted in their practical context. I hope this has given you a starting point for analysing and interpreting the results of A/B testing and similar&nbsp;data.</p>
<p>Finally, the full code used to create the figures in this post is located in this <a href="https://gist.github.com/t-redactyl/c7a57f53ac1e4145d11b">gist on my Github page</a>.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="http://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="http://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>