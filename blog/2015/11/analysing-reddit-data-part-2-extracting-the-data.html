<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Analysing reddit data - part 2: extracting the data</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Analysing reddit data - part 2: extracting the data written November 25, 2015 in python,programming tips,public data,reddit api,pandas">
  <meta name="keywords" content="python, programming, reproducible research" />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2015/11/analysing-reddit-data-part-2-extracting-the-data.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Analysing reddit data - part 2: extracting the data</h1>
        <div class="meta">
          written <time datetime="2015-11-25T00:00:00+01:00">November 25, 2015</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/python.html">python</a>,            <a href="https://t-redactyl.github.io/tag/programming-tips.html">programming tips</a>,            <a href="https://t-redactyl.github.io/tag/public-data.html">public data</a>,            <a href="https://t-redactyl.github.io/tag/reddit-api.html">reddit api</a>,            <a href="https://t-redactyl.github.io/tag/pandas.html">pandas</a>          </span>
        </div>
        <p>In <a href="https://t-redactyl.github.io/blog/2015/11/analysing-reddit-data-part-1-setting-up-the-environment.html">last week&#8217;s post</a>, we covered the basics of setting up our environment so we can extract data from reddit. Now it&#8217;s time to start on the meat of this topic. This week I will show you how to use the reddit public <span class="caps">API</span> to retrieve <span class="caps">JSON</span>-encoded data from the subreddit /r/relationships, although this technique will translate to both the reddit mainpage and other subreddits. As I mentioned last week, this is aimed at people who are completely new to working with <span class="caps">JSON</span> data, so we will go through everything&nbsp;step-by-step.</p>
<h2>Setting&nbsp;up</h2>
<p>To start, let&#8217;s set up our environment and start a new Jupyter notebook. Last week, I described how to set up a virtualenv for this project using <a href="http://fishshell.com/">Fish</a> and <a href="http://virtualfish.readthedocs.org/en/latest/index.html">virtualfish</a> in <span class="caps">OSX</span>. With this set up, we start by navigating to the folder we have created for this project (I recommend a local Git repo) and entering our&nbsp;virtualenv:</p>
<pre><code class="python">!cd ~/Documents/reddit_api
!vf activate reddit_api
</code></pre>

<p>Now that we are in the virtualenv, we launch&nbsp;Jupyter:</p>
<pre><code class="python">!ipython notebook
</code></pre>

<p>Jupyter will then open in your default browser. If you have an empty working directory, you will get something like&nbsp;this:</p>
<p><img src="/figure/Jupyter_screenshot_1.png" title="Jupyter home screen" alt="This is the blank Jupyter notebook homepage" style="display: block; margin: auto;" /></p>
<p>To start a new notebook, simply click on &#8216;New&#8217; and select &#8216;Python 2&#8217; under &#8216;Notebooks&#8217; in the resultant dropdown menu, as&nbsp;below:</p>
<p><img src="/figure/Jupyter_screenshot_2.png" title="Starting new Jupyter notebook" alt="This is how to start a new Jupyter notebook" style="display: block; margin: auto;" /></p>
<p>To get out of Jupyter when you are finished, close the browser window and enter control-c at the command&nbsp;line.</p>
<h2>How to retrieve <span class="caps">JSON</span>-encoded data from&nbsp;reddit</h2>
<p>Accessing the (publically-available) data from reddit is done using the <a href="https://www.reddit.com/dev/api">Reddit <span class="caps">API</span></a>. 
In this post, we will be looking at the most popular posts of all time on /r/relationships. We can access these using the <span class="caps">GET</span> request <a href="https://www.reddit.com/r/relationships/top/">https://www.reddit.com/r/relationships/top/</a>. Below is a sample of this page at the time I accessed&nbsp;it: </p>
<p><img src="/figure/rrel_top_normal.png" title="rrelationships top posts" alt="Capture of the first page of the top posts from subreddit relationships" style="display: block; margin: auto;" /></p>
<p>To retrieve the data in a <span class="caps">JSON</span>-encoded format, we simply add .json to the end of this request (i.e., <a href="https://www.reddit.com/r/relationships/top/.json">https://www.reddit.com/r/relationships/top/.json</a>). However, there is an issue with this request - by default, it is only giving us the top posts from the last 24 hours. You can see from the <span class="caps">API</span> documentation that this request is a listing, therefore it takes the parameters <code>after</code>, <code>before</code>, <code>limit</code>, <code>count</code> and <code>show</code>. There is also an additional parameter <code>t</code> not mentioned on this page, which limits the time period of the posts to show. So in order to get the top posts of all time, we need to change our request to <a href="https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all">https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all</a>. Additionally, if we want a specific number of posts (let&#8217;s start with one), we add the limit parameter to get <a href="https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all">https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all&amp;limit=1</a>. If we put this into our browser, we end up with the&nbsp;following:</p>
<p><img src="/figure/rrel_top_json.png" title="rrelationships top post json" alt="JSON-encoded data from the top post on subreddit relationships" style="display: block; margin: auto;" /></p>
<p>Gahh, what happened here?! I am sure you can see snippets of the data you want in there but it&#8217;s a mess. In order to start decoding this, let&#8217;s move over to reading the data in with Python. We first need to load in the required&nbsp;modules:</p>
<pre><code class="python">import urllib2
import json
</code></pre>

<p>We next need to send a request for the data we want. Before we can do this, we need have a look at the <a href="https://github.com/reddit/reddit/wiki/API">rules</a> that reddit has set up for accessing their site. The two most important rules for our current exercise is a) that we need to create a unique <a href="https://en.wikipedia.org/wiki/User_agent">UserAgent</a> string, and b) that we need to limit the number of requests we send to less than 30 a minute (i.e., one every two seconds). reddit requires that the UserAgent string contains your username, so if you don&#8217;t have a reddit account you will need to <a href="https://www.reddit.com/register">sign up</a> so you can get&nbsp;one.</p>
<p>We are now ready to send our&nbsp;request:</p>
<pre><code class="python">hdr = {'User-Agent': 'osx:r/relationships.single.result:v1.0 (by /u/&lt;PutYourUserNameHere&gt;)'}
url = 'https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all&amp;limit=1'
req = urllib2.Request(url, headers=hdr)
text_data = urllib2.urlopen(req).read()
</code></pre>

<p>In order to convert this into <span class="caps">JSON</span>-encoded data, we need to run the loads method from the json module on&nbsp;it:</p>
<pre><code class="python">data = json.loads(text_data)
</code></pre>

<p>If we call <code>dir</code> on the <code>data</code> object we just created, we can see that there is a method called <code>values</code>. If we call the values method on <code>data</code>, we can see that the contents of the post are contained in a series of nested dictionaries and&nbsp;lists.</p>
<pre><code class="python">data.values()
</code></pre>

<pre><code>[u'Listing',
 {u'after': u't3_3hw1jh',
  u'before': None,
  u'children': [{u'data': {u'approved_by': None,
     u'archived': False,
     u'author': u'whenlifegivesyoushit',
     u'author_flair_css_class': None,
     u'author_flair_text': None,
     u'banned_by': None,
     u'clicked': False,
     u'created': 1440191222.0,
     u'created_utc': 1440187622.0,
     u'distinguished': None,
     u'domain': u'self.relationships',
     u'downs': 0,
     u'edited': 1443809894.0,
     u'from': None,
     u'from_id': None,
     u'from_kind': None,
     u'gilded': 11,
     u'hidden': False,
     u'hide_score': False,
     u'id': u'3hw1jh',
     u'is_self': True,
     u'likes': None,
     u'link_flair_css_class': u'm-it updates',
     u'link_flair_text': u'Updates',
     u'locked': False,
     u'media': None,
     u'media_embed': {},
     u'mod_reports': [],
     u'name': u't3_3hw1jh',
     u'num_comments': 903,
     u'num_reports': None,
     u'over_18': False,
     u'permalink': u'/r/relationships/comments/3hw1jh/updatemy_26_f_with_my_husband_29_m_1_year_he_has/',
     u'quarantine': False,
     u'removal_reason': None,
     u'report_reasons': None,
     u'saved': False,
     u'score': 7751,
     u'secure_media': None,
     u'secure_media_embed': {},
     u'selftext': [truncated],
     u'selftext_html': [truncated],
     u'stickied': False,
     u'subreddit': u'relationships',
     u'subreddit_id': u't5_2qjvn',
     u'suggested_sort': None,
     u'thumbnail': u'',
     u'title': u'[UPDATE]My [26 F] with my husband [29 M] 1 year, he has been diagnosed with terminal cancer, how to make it count?',
     u'ups': 7751,
     u'url': u'https://www.reddit.com/r/relationships/comments/3hw1jh/updatemy_26_f_with_my_husband_29_m_1_year_he_has/',
     u'user_reports': [],
     u'visited': False},
    u'kind': u't3'}],
  u'modhash': u''}]
</code></pre>
<p>If, for example, we wanted to access the title of the post, it is nested within the following structure: [&#8216;Listing&#8217;, {&#8216;children&#8217;: [{&#8216;data&#8217;: {&#8216;title&#8217;:}}]}]. In other words, the nesting structure goes as follows: list -&gt; dictionary -&gt; list -&gt; dictionary -&gt; dictionary. We can therefore access it using a combination of list and dictionary&nbsp;indexing:</p>
<pre><code class="python">data.values()[1]['children'][0]['data']['title']
</code></pre>

<pre><code>u'[UPDATE]My [26 F] with my husband [29 M] 1 year, he has been diagnosed with terminal cancer, how to make it count?'
</code></pre>
<p>As all of the other data we want is in the same dictionary as <code>title</code>, we simply need to exchange the <code>title</code> key for the relevant key. For example, the total post score is under the <code>score</code> key. Getting the specific data you might want from reddit is a matter of going through and matching the information in the dictionary to data in the post to figure out what is&nbsp;what.</p>
<p>We now know how to extract information from a single post. Let&#8217;s move up to extracting information from a larger number of&nbsp;posts.</p>
<h2>Getting and storing larger quantities of&nbsp;data</h2>
<p>The methods above can be easily generalised in order to obtain larger quantities of data from reddit. However, there is one further hurdle to overcome. As you can see from the documentation, reddit only allows you to pull 100 posts at a time from the top board of a subreddit. We need some way of tracking the last post we get from each request and then starting the next request after this post. Luckily, each post has a unique identifier which is stored in the <code>name</code> key:</p>
<pre><code class="python">data.values()[1]['children'][0]['data']['name']
</code></pre>

<pre><code>u't3_3hw1jh'
</code></pre>
<p>We simply couple this with the <code>after</code> parameter in a loop in order to get the number of posts we want. In other words, we will send a request for the first 100 posts, obtain the identifier of the final post in that request, and ask that the next request of 100 start at the first post after this post. In our case, let&#8217;s extract the first 1000 posts. As part of extracting the data, we will keep only the content assigned to the <code>children</code> key in the first dictionary. This makes it possible to put all of the separate requests together in a collection (in this case, a list). In order to make sure we don&#8217;t accidentally exceed reddit&#8217;s request limit of 30 requests per minute, we&#8217;ll use the <code>sleep</code> method from the <code>time</code> module to place a pause of 2 seconds in between each iteration of the&nbsp;loop.</p>
<pre><code class="python">import time

hdr = {'User-Agent': 'osx:r/relationships.multiple.results:v1.0 (by /u/&lt;PutYourUserNameHere&gt;)'}
url = 'https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all&amp;limit=100'
req = urllib2.Request(url, headers=hdr)
text_data = urllib2.urlopen(req).read()
data = json.loads(text_data)
data_all = data.values()[1]['children']

while (len(data_all) &lt;= 1000):
    time.sleep(2)
    last = data_all[-1]['data']['name']
    url = 'https://www.reddit.com/r/relationships/top/.json?sort=top&amp;t=all&amp;limit=100&amp;after=%s' % last
    req = urllib2.Request(url, headers=hdr)
    text_data = urllib2.urlopen(req).read()
    data = json.loads(text_data)
    data_all += data.values()[1]['children']
</code></pre>

<p>Let&#8217;s check that we have retrieved the correct number of posts by checking the length of our&nbsp;list:</p>
<pre><code class="python">len(data_all)
</code></pre>

<pre><code>1000
</code></pre>
<p>Now that we have our data in raw <span class="caps">JSON</span> format, we can simply use another loop to extract the desired information from each post. In this case, I have decided to get the date of posting, title, flair, number of comments and total&nbsp;score.</p>
<pre><code class="python">article_title = []
article_flairs = []
article_date = []
article_comments = []
article_score = []

for i in range(0, len(data_all)):
    article_title.append(data_all[i]['data']['title'])
    article_flairs.append(data_all[i]['data']['link_flair_text'])
    article_date.append(data_all[i]['data']['created_utc'])
    article_comments.append(data_all[i]['data']['num_comments'])
    article_score.append(data_all[i]['data']['score'])
</code></pre>

<p>Now we&#8217;re pretty much there! The final step is creating a <code>pandas DataFrame</code> using these lists of&nbsp;results.</p>
<pre><code class="python">import numpy as np
from pandas import Series, DataFrame
import pandas as pd

rel_df = DataFrame({'Date': article_date,
                    'Title': article_title,
                    'Flair': article_flairs,
                    'Comments': article_comments,
                    'Score': article_score})
rel_df = rel_df[['Date', 'Title', 'Flair', 'Comments', 'Score']]
</code></pre>

<pre><code class="python">rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1440187622</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>903</td>
      <td>7755</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1438962646</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>631</td>
      <td>6013</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1435026034</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5519</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1438393090</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>972</td>
      <td>5295</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1440543117</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>768</td>
      <td>5181</td>
    </tr>
  </tbody>
</table>
</div>

<p>We now have a <code>pandas DataFrame</code> that is ready for cleaning and analysis. Next week I will start with both cleaning problematic variables (e.g., converting <code>Date</code> into a datetime format and dealing with the &#8220;Locked Post&#8221; flairs) and extracting further data from these variables. I will finish this series doing some analyses and plotting with these&nbsp;data.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>