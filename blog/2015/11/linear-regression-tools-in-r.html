<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Linear regression tools in R</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Linear regression tools in R written November 04, 2015 in statistics,r,regression,programming tips">
  <meta name="keywords" content="rlanguage, data science, hypothesis testing, linear regression" />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2015/11/linear-regression-tools-in-r.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Linear regression tools in R</h1>
        <div class="meta">
          written <time datetime="2015-11-04T00:00:00+01:00">November 04, 2015</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/statistics.html">statistics</a>,            <a href="https://t-redactyl.github.io/tag/r.html">r</a>,            <a href="https://t-redactyl.github.io/tag/regression.html">regression</a>,            <a href="https://t-redactyl.github.io/tag/programming-tips.html">programming tips</a>          </span>
        </div>
        <p>Choosing the right <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a> model for your data can be an overwhelming venture, especially when you have a large number of available predictors. Luckily R has a wide array of in-built and user-written tools to make this process easier. In this week&#8217;s blog post I will describe some of the tools I commonly&nbsp;use.</p>
<p>For illustration I will use the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">mtcars</a> dataset. The first step as always is to load in the&nbsp;data.</p>
<pre><code class="r">rm(list = ls())
data(mtcars)
</code></pre>

<p>I will also do a little bit of data cleaning by creating labelled factor variables for the categorical&nbsp;variables.</p>
<pre><code class="r">mtcars$am.f &lt;- as.factor(mtcars$am); levels(mtcars$am.f) &lt;- c(&quot;Automatic&quot;, &quot;Manual&quot;)
mtcars$cyl.f &lt;- as.factor(mtcars$cyl); levels(mtcars$cyl.f) &lt;- c(&quot;4 cyl&quot;, &quot;6 cyl&quot;, &quot;8 cyl&quot;)
mtcars$vs.f &lt;- as.factor(mtcars$vs); levels(mtcars$vs.f) &lt;- c(&quot;V engine&quot;, &quot;Straight engine&quot;)
mtcars$gear.f &lt;- as.factor(mtcars$gear); levels(mtcars$gear.f) &lt;- c(&quot;3 gears&quot;, &quot;4 gears&quot;, &quot;5 gears&quot;)
mtcars$carb.f &lt;- as.factor(mtcars$carb)
</code></pre>

<p>We&#8217;re now ready to go through our model building tools. Similarly to <a href="https://t-redactyl.github.io/blog/2015/10/interpreting-linear-regression-coefficients.html">last week&#8217;s blog post</a>, I want to examine how well transmission type (<code>am</code>) predicts a car&#8217;s miles per gallon (<code>mpg</code>), taking into account other covariates and their interactions as&nbsp;appropriate.</p>
<h2>Normality, linearity and&nbsp;multicollinearity</h2>
<p>The first step to building the model is checking whether the data meet the assumptions of linear regression. A really neat way to simultaneously check the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normality</a> of the outcome, the <a href="https://en.wikipedia.org/wiki/Linearity">linearity</a> of the relationships between the outcome and the predictors and the <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence">intercorrelations</a> between the predictors is the <strong>ggpairs</strong> plot in the very handy <a href="https://cran.r-project.org/web/packages/GGally/GGally.pdf">GGally R package</a>. Before we run the ggpairs plot, I&#8217;ll rearrange the dataframe so the columns are in a more useful order for scanning the plot (with <code>mpg</code> and <code>am</code> as the first&nbsp;columns).</p>
<pre><code class="r">library(ggplot2); library(GGally)
mtcars &lt;- mtcars[ , c(1, 9, 2:8, 10:16)]
g = ggpairs(mtcars[ , 1:11], lower = list(continuous = &quot;smooth&quot;, params = c(method = &quot;loess&quot;)))
g
</code></pre>

<p><img alt="plot of chunk ggpairs_plot" src="/figure/ggpairs_plot-1.png"> </p>
<p>Note that this plot doesn&#8217;t work with factor variables, and the only categorical variables that should be included are either binary or ordinal. We can see that <code>mpg</code> is roughly normal (albeit a little positively skewed), and that the continuous variables and ordinal variables have linear relationship with <code>mpg</code>. We can also see there are a few very high intercorrelations between the potential predictor variables, but it is a little hard to pick these out in the volume of&nbsp;information.</p>
<p>A quick and easy alternative for finding <a href="https://en.wikipedia.org/wiki/Multicollinearity">collinear</a> pairs is using the <code>spec.cor</code> function written by <a href="http://joshuawiley.com/">Joshua Wiley</a>. This handy little function allows you to set a cutoff correlation level (in this case, 0.8), and it will return all pairs that correlate at or above that&nbsp;level.</p>
<pre><code class="r">spec.cor &lt;- function (dat, r, ...) { 
    x &lt;- cor(dat, ...) 
    x[upper.tri(x, TRUE)] &lt;- NA 
    i &lt;- which(abs(x) &gt;= r, arr.ind = TRUE) 
    data.frame(matrix(colnames(x)[as.vector(i)], ncol = 2), value = x[i]) 
} 

spec.cor(mtcars[ , 2:11], 0.8)
</code></pre>

<pre><code>##     X1   X2      value
## 1 disp  cyl  0.9020329
## 2   hp  cyl  0.8324475
## 3   vs  cyl -0.8108118
## 4   wt disp  0.8879799
</code></pre>

<p>In this case we can see we have four collinear pairs of predictors. I&#8217;ve decided I will keep <code>wt</code> and discard <code>disp</code>, <code>hp</code> and <code>vs</code>. My choice was based on simple predictor reduction as I don&#8217;t know anything about the content area and variables, but I might have made a different decision (e.g., keeping <code>hp</code> and <code>vs</code>) if one of those variables was particularly important or&nbsp;interpretable.</p>
<h2>Predictor&nbsp;selection</h2>
<p>The next step is selecting the predictors to include in the model alongside <code>am</code>. As I don&#8217;t know anything about the predictors, I will select to enter them into the model based purely on their relationship with the outcome (with higher correlations meaning they will be entered sooner). I wrote a function below which correlates each predictor with the outcome and ranks them (as absolute values) in descending&nbsp;order.</p>
<pre><code class="r">cor.list &lt;- c()
outcome.cor &lt;- function(predictor, ...) {
    x &lt;- cor(mtcars$mpg, predictor)
    cor.list &lt;- c(cor.list, x)
}
cor.list &lt;- sapply(mtcars[ , c(3, 6:8, 10:11)], outcome.cor)
sort(abs(cor.list), decreasing = TRUE)
</code></pre>

<pre><code>##        wt       cyl      drat      carb      gear      qsec 
## 0.8676594 0.8521620 0.6811719 0.5509251 0.4802848 0.4186840
</code></pre>

<p>You can see the predictor that has the strongest bivariate relationship with <code>mpg</code> is <code>wt</code>, then <code>cyl</code>, etc. We will use this order to enter variables into our model on top of <code>am</code>. One way of working out if adding a new variable improves the fit of the model is comparing models using the <code>anova</code> function. This function compares two nested models and returns the F-change and its associated significance level when adding the new&nbsp;variable(s). </p>
<p>When building the nested models, I will add one main effect at a time, following the bivariate relationship strength between each predictor and the outcome. I have written a function below which tests pairs of nested models and stores the two models and the significance of the F-change in a dataframe to make it easy to check whether a change improves the model fit. You can see I have run the models one by one and checked them, and only retained variables that improved model&nbsp;fit.</p>
<pre><code class="r">lmfits &lt;- data.frame()
lmfit.table &lt;- function(model1, model2, ...) {
    models &lt;- sub(&quot;Model 1: &quot;, &quot;&quot;, attr(anova(model1, model2), &quot;heading&quot;)[2])
    x &lt;- c(sub(&quot;\\n.*&quot;, &quot;&quot;, models),
           sub(&quot;.*Model 2: &quot;, &quot;&quot;, models), 
           round(anova(model1, model2)$&quot;Pr(&gt;F)&quot;[2], 3))
    lmfits &lt;- rbind(lmfits, x)
}

model1 &lt;- lm(mpg ~ am.f, data = mtcars)
model2 &lt;- lm(mpg ~ am.f + wt, data = mtcars)

lmfits &lt;- lmfit.table(model1, model2)
for (i in 1:3) {
    lmfits[ , i] &lt;- as.character(lmfits[ , i])
}
names(lmfits) &lt;- c(&quot;Model 1&quot;, &quot;Model 2&quot;, &quot;p-value of model improvement&quot;)

model3 &lt;- lm(mpg ~ am.f + wt + cyl.f, data = mtcars)
lmfits &lt;- lmfit.table(model2, model3)

model4 &lt;- lm(mpg ~ am.f + wt + cyl.f + drat, data = mtcars)
lmfits &lt;- lmfit.table(model3, model4)

model5 &lt;- lm(mpg ~ am.f + wt + cyl.f + carb.f, data = mtcars)
lmfits &lt;- lmfit.table(model3, model5)

model6 &lt;- lm(mpg ~ am.f + wt + cyl.f + gear.f, data = mtcars)
lmfits &lt;- lmfit.table(model3, model6)

model7 &lt;- lm(mpg ~ am.f + wt + cyl.f + qsec, data = mtcars)
lmfits &lt;- lmfit.table(model3, model7)

require(knitr)
lmfits
</code></pre>

<div>
<table class="table table-bordered">
    <thead>
        <tr style="text-align: right;">
            <th>Model 1</th>
            <th>Model 2</th>
            <th>p-value of model improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>mpg ~ am.f</td>
            <td>mpg ~ am.f + wt</td>
            <td>0</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt</td>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>0.003</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>mpg ~ am.f + wt + cyl.f + drat</td>
            <td>0.861</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>mpg ~ am.f + wt + cyl.f + carb.f</td>
            <td>0.764</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>mpg ~ am.f + wt + cyl.f + gear.f</td>
            <td>0.564</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>mpg ~ am.f + wt + cyl.f + qsec</td>
            <td>0.061</td>
        </tr>
    </tbody>
</table>
</div>

<p>There were two variables that improved model fit in addition to <code>am</code>, which were <code>wt</code> and <code>cyl</code>. I will now check whether adding interaction terms between these variables and these variables improves model&nbsp;fit:</p>
<pre><code class="r">model8 &lt;- lm(mpg ~ am.f + wt + cyl.f + am.f * wt, data = mtcars)
lmfits &lt;- lmfit.table(model3, model8)

model9 &lt;- lm(mpg ~ am.f + wt + cyl.f + am.f * wt + am.f * cyl.f, data = mtcars)
lmfits &lt;- lmfit.table(model8, model9)

model10 &lt;- lm(mpg ~ am.f + wt + am.f * wt, data = mtcars)
lmfits &lt;- lmfit.table(model2, model10)

lmfits[7:9, ]
</code></pre>

<div>
<table class="table table-bordered">
    <thead>
        <tr style="text-align: right;">
            <th>Model 1</th>
            <th>Model 2</th>
            <th>p-value of model improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f</td>
            <td>mpg ~ am.f + wt + cyl.f + am.f * wt</td>
            <td>0.007</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt + cyl.f + am.f * wt</td>
            <td>mpg ~ am.f + wt + cyl.f + am.f * wt + am.f * cyl.f</td>
            <td>0.802</td>
        </tr>
        <tr>
            <td>mpg ~ am.f + wt</td>
            <td>mpg ~ am.f + wt + am.f * wt</td>
            <td>0.001</td>
        </tr>
    </tbody>
</table>
</div>

<p>We now have two viable models, <code>model8</code> and <code>model10</code>. To select between these, I will have a look at the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><span class="math">\(R^2\)</span></a> and <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">variance inflation factor (<span class="caps">VIF</span>)</a> (in the <a href="https://cran.r-project.org/web/packages/car/car.pdf">car package</a>) of each of the&nbsp;models.</p>
<pre><code class="r">require(car)
round(summary(model10)$r.squared, 3)
</code></pre>

<pre><code>## [1] 0.833
</code></pre>

<pre><code class="r">vif(model10)
</code></pre>

<pre><code>##      am.f        wt   am.f:wt 
## 20.901259  2.728248 15.366853
</code></pre>

<pre><code class="r">round(summary(model8)$r.squared, 3)
</code></pre>

<pre><code>## [1] 0.877
</code></pre>

<pre><code class="r">vif(model8)[ , 1]
</code></pre>

<pre><code>##      am.f        wt     cyl.f   am.f:wt 
## 24.302147  3.983261  3.060691 18.189413
</code></pre>

<p>The difference in <span class="math">\(R^2\)</span> between the two models is small, but the inclusion of <code>cyl</code> in the model both increases the variance inflation and decreases the interpretability of the model. Moreover, <code>cyl</code> is highly correlated with <code>wt</code> (0.782), meaning it is likely explaining a lot of the same variance as <code>wt</code>. As such, I dropped <code>cyl</code> and the final model included <code>am</code>, <code>wt</code>, and their interaction&nbsp;term.</p>
<h2>Model&nbsp;diagnostics</h2>
<p>Having chosen the final model, it is time to check whether it has any issues with how it fits the data. The built-in <code>plot</code> function conveniently displays four <a href="https://en.wikipedia.org/wiki/Regression_diagnostic">diagnostic plots</a> for lm&nbsp;objects:</p>
<pre><code class="r">final.model &lt;- lm(mpg ~ am.f + wt + am.f * wt, data = mtcars)
par(mfrow = c(2,2))
plot(final.model)
</code></pre>

<p><img alt="plot of chunk diagnostic_plots" src="/figure/diagnostic_plots-1.png"> </p>
<p>The <strong>Residuals vs Fitted</strong> plot (and its standardised version, the <strong>Scale Location</strong> plot) show that higher <span class="caps">MPG</span> values tend to have higher <a href="https://en.wikipedia.org/wiki/Errors_and_residuals">residuals</a>. In addition, there are three values with unusually high residual error (Merc <span class="caps">240DD</span>, Fiat128 and Toyota Corolla), indicating that the model is a poor fit for both cars with high <span class="caps">MPG</span> (past about 28 <span class="caps">MPG</span>) and these three car types. The <strong>Normal Q-Q</strong> plot of residuals indicates that the errors are not normally distributed, again especially for high levels of <span class="caps">MPG</span> and the three specific car models that had high residuals. Finally, the <strong>Residuals vs Leverage</strong> plot demonstrates that are a couple of values with high <a href="https://en.wikipedia.org/wiki/Leverage_(statistics)">leverage</a> and low residuals, which may possibly be biasing the trend&nbsp;line.</p>
<p>To more closely examine the effect of variables with high leverage and/or <a href="https://en.wikipedia.org/wiki/Influential_observation">influence</a> on the regression line we can extract the hatvalues and the dfbeta values of the model respectively. First I will show the top 6&nbsp;hatvalues:</p>
<pre><code class="r">mtcars$name &lt;- row.names(mtcars)
mtcars$hatvalues &lt;- round(hatvalues(final.model), 3)
top.leverage = sort(round(hatvalues(final.model), 3), decreasing = TRUE)
head(top.leverage)
</code></pre>

<pre><code>##       Maserati Bora Lincoln Continental   Chrysler Imperial 
##               0.371               0.304               0.281 
##  Cadillac Fleetwood        Lotus Europa         Honda Civic 
##               0.254               0.253               0.216
</code></pre>

<p>The Maserati Bora appears to have a substantially higher leverage than the others in the list. Ok, so now let&#8217;s look at the&nbsp;dfbetas:</p>
<pre><code class="r">mtcars$dfbetas &lt;- round(dfbetas(final.model)[, 2], 3)
top.influence = sort(round(abs(dfbetas(final.model)[, 2]), 3), decreasing = TRUE)
head(top.influence)
</code></pre>

<pre><code>## Chrysler Imperial    Toyota Corolla      Lotus Europa         Merc 240D 
##             0.584             0.469             0.372             0.349 
##          Fiat 128          Merc 230 
##             0.327             0.231
</code></pre>

<p>Despite its high leverage, the Maserati Bora does not have high influence. Instead, the Chrysler Imperial and the Toyota Corolla are much higher than the other models. So how do we actually tell if these values are biasing the fit of the regression line? I wrote two ggplots which label the data points by name with the highest leverage and influence values. I chose the three highest, but you can easily change the code to give you whatever number you&nbsp;want.</p>
<pre><code class="r"># First build a plot of the model.
library(ggplot2)
gp &lt;- ggplot(data=mtcars, aes(x=wt, y=mpg, colour=am.f)) + 
        geom_point(alpha = 0.7) +
        geom_abline(intercept = coef(final.model)[1], slope = coef(final.model)[3], 
                    size = 1, color = &quot;#FF3721&quot;) +
        geom_abline(intercept = coef(final.model)[1] + coef(final.model)[2], 
                    slope = coef(final.model)[3] + coef(final.model)[4], 
                    size = 1, color = &quot;#4271AE&quot;) +
        scale_colour_manual(name=&quot;Transmission&quot;, values =c(&quot;#FF3721&quot;, &quot;#4271AE&quot;)) +
        ylab(&quot;Miles per gallon&quot;) +    
        xlab(&quot;Weight (`000 lbs)&quot;) +
        theme_bw()

# Leverage plot
g1 &lt;- gp + geom_text(data=subset(mtcars, abs(hatvalues) &gt;= top.leverage[3]), 
                     aes(wt,mpg,label=name), size = 4, hjust=1, vjust=0) +
        ggtitle(&quot;Three Points with Highest Leverage (hatvalues)&quot;)

# Influence plot
g2 &lt;- gp + geom_text(data=subset(mtcars, abs(dfbetas) == top.influence[1]), 
                     aes(wt,mpg,label=name), size = 4, hjust = 1, vjust = 0) +
        geom_text(data=subset(mtcars, abs(dfbetas) == top.influence[2] | 
                                  abs(dfbetas) == top.influence[3]), 
                     aes(wt,mpg,label=name), size = 4, hjust = 0, vjust = 0) +
        ggtitle(&quot;Three Points with Highest Influence (dfbetas)&quot;)

library(gridExtra)
grid.arrange(g1, g2, nrow = 1, ncol = 2)
</code></pre>

<p><img alt="plot of chunk outlier_plots" src="/figure/outlier_plots-1.png"> </p>
<p>You can see that none of the high leverage nor influence points seem to be biasing the fit of the regression&nbsp;lines.</p>
<p>I hope this post has given you some useful tools to streamline your linear regression model-building process. I hope you can also see that while all of the decisions I made were data-driven, several were ultimately subjective, and there were a few alternative models that may have worked equally well. This highlights the fact that the &#8220;right&#8221; model is dependent on what you&#8217;re going to use it for, the overall interpretability, and what the content expert tells you is&nbsp;important.</p>
<p><img src="/figure/interpretation-1.png" title="plot of chunk interpretation" alt="plot of chunk interpretation" style="display: block; margin: auto;" /></p>
<p>(Image via <a href="https://twitter.com/ResearchMark">Research Wahlberg</a>)</p>
<p>I have taken some of the information and code used in this post from the excellent <a href="https://www.coursera.org/course/regmods">Regression Models</a> unit on Coursera by <a href="https://twitter.com/bcaffo">Brian Caffo</a>, <a href="https://twitter.com/jtleek">Jeff Leek</a> and <a href="https://twitter.com/rdpeng">Roger Peng</a>, and the model used was derived from work I did for the class assignment for that&nbsp;course.</p>
<p>Finally, the full code used to create the figures in this post is located in this <a href="https://gist.github.com/t-redactyl/5e0e274cb65716260946">gist on my Github page</a>.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>