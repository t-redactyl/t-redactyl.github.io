<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Analysing reddit data - part 3: cleaning and describing the data</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Analysing reddit data - part 3: cleaning and describing the data written December 02, 2015 in python,programming tips,public data,reddit api,pandas">
  <meta name="keywords" content="python, programming, reproducible research, reddit api, json, pandas, regex, numpy" />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2015/12/analysing-reddit-data-part-3-cleaning-and-describing-the-data.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Analysing reddit data - part 3: cleaning and describing the data</h1>
        <div class="meta">
          written <time datetime="2015-12-02T00:00:00+01:00">December 02, 2015</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/python.html">python</a>,            <a href="https://t-redactyl.github.io/tag/programming-tips.html">programming tips</a>,            <a href="https://t-redactyl.github.io/tag/public-data.html">public data</a>,            <a href="https://t-redactyl.github.io/tag/reddit-api.html">reddit api</a>,            <a href="https://t-redactyl.github.io/tag/pandas.html">pandas</a>          </span>
        </div>
        <p>Over the past two weeks (<a href="https://t-redactyl.github.io/blog/2015/11/analysing-reddit-data-part-1-setting-up-the-environment.html">here</a> and <a href="https://t-redactyl.github.io/blog/2015/11/analysing-reddit-data-part-2-extracting-the-data.html">here</a>) we have been discussing how to use <span class="caps">JSON</span>-encoded data from reddit. So far we have set up our environment and extracted the top 1,000 posts of all time from the subreddit <a href="https://www.reddit.com/r/relationships#hme">/r/relationships</a> into a <code>pandas Dataframe</code>. This week, we will work on cleaning the data, extracting further data from our existing variables and describing these variables. We&#8217;ll end this series next week by doing some basic inferential&nbsp;analyses.</p>
<h2>Picking up where we left&nbsp;off</h2>
<p>Last week, we ended up with a <code>pandas Dataframe</code> called <code>rel_df</code> with five variables: <code>Date</code>, <code>Title</code>, <code>Flair</code>, <code>Comments</code> and <code>Score</code>. If you don&#8217;t have this Dataframe prepared, you&#8217;ll need to go back to the previous posts and set this up to continue with the tutorial. Here is the first 5 results from this&nbsp;Dataframe.</p>
<pre><code class="python">rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1440187622</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>908</td>
      <td>7843</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1438962646</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>631</td>
      <td>6038</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1435026034</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5548</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1438393090</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>970</td>
      <td>5301</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1440543117</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>767</td>
      <td>5195</td>
    </tr>
  </tbody>
</table>
</div>

<h2>Cleaning the&nbsp;data</h2>
<p>As I pointed out in the last blog post, there are two immediately obvious issues with these data. The first is that the date is in <a href="https://en.wikipedia.org/wiki/Unix_time">Unix or Epoch time</a>, which represents the number of seconds that have passed since 1 January 1970. In order to convert this into a datetime format, we run the&nbsp;following:</p>
<pre><code class="python">rel_df['Date'] = pd.to_datetime((rel_df['Date'].values*1e9).astype(int))
rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-08-21 20:07:02</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>908</td>
      <td>7843</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-08-07 15:50:46</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>631</td>
      <td>6038</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-06-23 02:20:34</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5548</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-08-01 01:38:10</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>◉ Locked Post ◉</td>
      <td>970</td>
      <td>5301</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-08-25 22:51:57</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>767</td>
      <td>5195</td>
    </tr>
  </tbody>
</table>
</div>

<p>The other issue is that when posts become locked by the subreddit moderators, the original flair is replaced with &#8220;Locked Post&#8221;. This is not really the most useful label as it doesn&#8217;t give us any information about the topic. Let&#8217;s replace all of the &#8220;Locked Post&#8221; flairs with missing values (NaN) and have a look at how many there&nbsp;are.</p>
<pre><code class="python">import re

replace_value = rel_df['Flair'][1]
rel_df['Flair'] = rel_df['Flair'].replace(replace_value, np.nan)

rel_df['Flair'].isnull().sum()
</code></pre>

<pre><code>155
</code></pre>
<p>You can see a substantial number (16%) of flairs were replaced with &#8220;Locked Post&#8221;, which means we have a large amount of <a href="https://en.wikipedia.org/wiki/Missing_data">missing data</a> in this variable. However, we can recover some information by exploiting the fact that update posts usually have the word &#8220;Update&#8221; in the title. We can use a <a href="https://en.wikipedia.org/wiki/Regular_expression">regex</a> to check for whether &#8220;Update&#8221; is in the title, and if so, replace the flair with&nbsp;&#8220;Updates&#8221;.</p>
<pre><code class="python">cond1 = rel_df['Title'].str.contains(
    '^\[?[a-z!?A-Z ]*UPDATE\]?:?', flags = re.IGNORECASE)
cond2 = rel_df['Flair'].isnull()

rel_df.loc[(cond1 &amp; cond2), 'Flair'] = rel_df.loc[(cond1 &amp; cond2), 'Flair'].replace(np.nan, 'Updates')
rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-08-21 20:07:02</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>908</td>
      <td>7843</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-08-07 15:50:46</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>Updates</td>
      <td>631</td>
      <td>6038</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-06-23 02:20:34</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5548</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-08-01 01:38:10</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>NaN</td>
      <td>970</td>
      <td>5301</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-08-25 22:51:57</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>767</td>
      <td>5195</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">rel_df['Flair'].isnull().sum()
</code></pre>

<pre><code>96
</code></pre>
<p>You can see that we still have 10% of missing data in this variable, which is quite high. However, we have replaced some of the missing data in a robust manner. This should be bookmarked as a possible source of <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">bias</a> in our data when we try and interpret our&nbsp;analyses.</p>
<h2>Extracting extra&nbsp;variables</h2>
<p>You might have gotten the idea from how I replaced the missing flairs that we could extract further information from the <code>Title</code> variable, and indeed we can. Another convention that we can exploit is that posters to /r/relationships are asked to include their age and sex in the title. For example, you can see in the first post that the poster has included their age and sex as &#8220;[26F]&#8221;, indicating they are a 26 year old woman. You can probably also see a pattern in how the posters information is nested in the title as well. Looking through the data, I picked out four words that precede the posters&#8217; information: &#8220;My&#8221;, &#8220;I&#8221;, &#8220;I&#8217;m&#8221; and &#8220;Me&#8221;. We can use a (pretty complicated) regex to extract this portion of the&nbsp;title:</p>
<pre><code class="python">poster_age_sex = rel_df['Title'].str.extract(
    &quot;((i\'m|i|my|me)\s?(\[|\()(m|f)?(\s|/)?[0-9]{1,2}(\s|/)?([m,f]|male|female)?(\]|\)))&quot;, 
        flags = re.IGNORECASE)[0]
poster_age_sex[:5]
</code></pre>

<pre><code>0    My [26 F]
1     I [30 F]
2      I (25m)
3     My [42M]
4     My [26F]
Name: 0, dtype: object
</code></pre>
<p>Let&#8217;s now clean this up by getting rid of the starting word, then pulling the age and sex out into separate variables and adding them to the&nbsp;DataFrame.</p>
<pre><code class="python">poster_age_sex = poster_age_sex.str.replace(&quot;((i\'m|i|my|me))\s?&quot;, &quot;&quot;, flags = re.IGNORECASE)
poster_age = poster_age_sex.str.extract('([0-9]{1,2})')
poster_sex = poster_age_sex.str.extract('([m,f])', flags = re.IGNORECASE)

rel_df['PosterAge'] = pd.to_numeric(poster_age)
rel_df['PosterSex'] = poster_sex.str.upper()

rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
      <th>PosterAge</th>
      <th>PosterSex</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-08-21 20:07:02</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>908</td>
      <td>7843</td>
      <td>26</td>
      <td>F</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-08-07 15:50:46</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>Updates</td>
      <td>631</td>
      <td>6038</td>
      <td>30</td>
      <td>F</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-06-23 02:20:34</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5548</td>
      <td>25</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-08-01 01:38:10</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>NaN</td>
      <td>970</td>
      <td>5301</td>
      <td>42</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-08-25 22:51:57</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>767</td>
      <td>5195</td>
      <td>26</td>
      <td>F</td>
    </tr>
  </tbody>
</table>
</div>

<p>Let&#8217;s now check for missing values in our new PosterAge and PosterSex&nbsp;variables:</p>
<pre><code class="python">rel_df['PosterAge'].isnull().sum()
</code></pre>

<pre><code>91
</code></pre>
<pre><code class="python">rel_df['PosterSex'].isnull().sum()
</code></pre>

<pre><code>103
</code></pre>
<p>Again, these variables have fairly high amounts of missing data (9% for age 10% for sex). This is another possible source of bias to keep in mind. I&#8217;ll discuss how these possible biases might affect how we interpret our analyses at the end of next week&#8217;s&nbsp;post.</p>
<p>Finally, we can use the date variable to obtain the day of the week that the post was&nbsp;created:</p>
<pre><code class="python">rel_df['DayOfWeek'] = rel_df['Date'].dt.dayofweek
days = {0: 'Mon', 1: 'Tues', 2: 'Weds', 3: 'Thurs', 4: 'Fri',
        5: 'Sat', 6: 'Sun'}
rel_df['DayOfWeek'] = rel_df['DayOfWeek'].apply(lambda x: days[x])

rel_df[:5]
</code></pre>

<div>
<table class="table table-bordered">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Title</th>
      <th>Flair</th>
      <th>Comments</th>
      <th>Score</th>
      <th>PosterAge</th>
      <th>PosterSex</th>
      <th>DayOfWeek</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2015-08-21 20:07:02</td>
      <td>[<span class="caps">UPDATE</span>]My [26 F] with my husband [29 M] 1 yea&#8230;</td>
      <td>Updates</td>
      <td>908</td>
      <td>7843</td>
      <td>26</td>
      <td>F</td>
      <td>Fri</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2015-08-07 15:50:46</td>
      <td>Update: I [30 F] am sitting in the back of my &#8230;</td>
      <td>Updates</td>
      <td>631</td>
      <td>6038</td>
      <td>30</td>
      <td>F</td>
      <td>Fri</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2015-06-23 02:20:34</td>
      <td><span class="caps">UPDATE</span>: My fiancee (24F) has no bridesmaids an&#8230;</td>
      <td>Updates</td>
      <td>623</td>
      <td>5548</td>
      <td>25</td>
      <td>M</td>
      <td>Tues</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2015-08-01 01:38:10</td>
      <td>My [42M] daughter [17F] has been bullying a gi&#8230;</td>
      <td>NaN</td>
      <td>970</td>
      <td>5301</td>
      <td>42</td>
      <td>M</td>
      <td>Sat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2015-08-25 22:51:57</td>
      <td>[Update] My [26F] fiance&#8217;s [28M] ex-wife [28F]&#8230;</td>
      <td>Updates</td>
      <td>767</td>
      <td>5195</td>
      <td>26</td>
      <td>F</td>
      <td>Tues</td>
    </tr>
  </tbody>
</table>
</div>

<p>Checking for missing data, we find that this time no values are missing (due to the fact that every date is&nbsp;present).</p>
<pre><code class="python">rel_df['DayOfWeek'].isnull().sum()
</code></pre>

<pre><code>0
</code></pre>
<h2>Descriptives</h2>
<p>Now that we have finished cleaning, we are now ready to get acquainted with our data using some <a href="https://en.wikipedia.org/wiki/Descriptive_statistics">descriptives</a>. For the sake of brevity I will skip screening for <a href="https://en.wikipedia.org/wiki/Normal_distribution">normality</a> and assume all of the continuous variables are non-normal, but obviously in a real analysis it would be necessary to explore this further. As such, I will use the <a href="https://en.wikipedia.org/wiki/Median">median</a> and <a href="https://en.wikipedia.org/wiki/Interquartile_range">interquartile range (<span class="caps">IQR</span>)</a> for continuous variables, and frequencies and percentages for categorical&nbsp;variables.</p>
<h3>Poster&nbsp;age</h3>
<p>Let&#8217;s start with <code>PosterAge</code>. We can see we have 909 data points for this variable, and that posters are a median of 26 years old (<span class="caps">IQR</span>: 23,&nbsp;29).</p>
<pre><code class="python">rel_df['PosterAge'].describe()
</code></pre>

<pre><code>count    909.000000
mean      26.698570
std        6.323285
min       13.000000
25%       23.000000
50%       26.000000
75%       29.000000
max       57.000000
Name: PosterAge, dtype: float64
</code></pre>
<h3>Poster&nbsp;sex</h3>
<p>Looking at <code>PosterSex</code>, we can see that we have 897 data points for this variable. 542 of the posters are female (60% of non-missing values), and 355 are male&nbsp;(40%).</p>
<pre><code class="python">rel_df['PosterSex'].notnull().sum()
</code></pre>

<pre><code>897
</code></pre>
<pre><code class="python">rel_df['PosterSex'].value_counts()
</code></pre>

<pre><code>F    542
M    355
Name: PosterSex, dtype: int64
</code></pre>
<pre><code class="python">100 * rel_df['PosterSex'].value_counts() / rel_df['PosterSex'].notnull().sum()
</code></pre>

<pre><code>F    60.423634
M    39.576366
Name: PosterSex, dtype: float64
</code></pre>
<h3>Flairs</h3>
<p>In <code>Flairs</code>, we have 904 complete data points. The most common flair is &#8220;Updates&#8221; (516 posts, or 57%), and the least common is &#8220;Dating&#8221; (3, &gt; 1%). The bottom three categories are concerningly small and are therefore unlikely to be suitable for further analysis, especially when we get to doing subgroup analyses next week (see this <a href="https://t-redactyl.github.io/blog/2015/09/representative-sampling.html">previous blog post</a> for a discussion on the importance of sufficiently large&nbsp;samples).</p>
<pre><code class="python">rel_df['Flair'].notnull().sum()
</code></pre>

<pre><code>904
</code></pre>
<pre><code class="python">rel_df['Flair'].value_counts()
</code></pre>

<pre><code>Updates            516
Relationships      161
Non-Romantic       158
Infidelity          38
Breakups            15
Personal issues     13
Dating               3
Name: Flair, dtype: int64
</code></pre>
<pre><code class="python">100 * rel_df['Flair'].value_counts() / rel_df['Flair'].notnull().sum()
</code></pre>

<pre><code>Updates            57.079646
Relationships      17.809735
Non-Romantic       17.477876
Infidelity          4.203540
Breakups            1.659292
Personal issues     1.438053
Dating              0.331858
Name: Flair, dtype: float64
</code></pre>
<h3>Score</h3>
<p>Examining the <code>Score</code> variable we can see that it has all 1,000 data points, and the median score per post is 1,225 (<span class="caps">IQR</span>: 961,&nbsp;1,761).</p>
<pre><code class="python">rel_df['Score'].describe()
</code></pre>

<pre><code>count    1000.00000
mean     1511.58000
std       822.78436
min       792.00000
25%       963.00000
50%      1224.50000
75%      1762.00000
max      7843.00000
Name: Score, dtype: float64
</code></pre>
<h3>Comments</h3>
<p>Similarly, the <code>Comments</code> variable has all 1,000 data points. The median number of comments per post is 269 (<span class="caps">IQR</span>: 161,&nbsp;421).</p>
<pre><code class="python">rel_df['Comments'].describe()
</code></pre>

<pre><code>count    1000.000000
mean      318.964000
std       219.461632
min        15.000000
25%       161.000000
50%       269.000000
75%       421.250000
max      1693.000000
Name: Comments, dtype: float64
</code></pre>
<h3>Day of&nbsp;week</h3>
<p>Finally, let&#8217;s have a look at <code>DayOfWeek</code>. We already know it has all 1,000 data points, so we don&#8217;t have to check that again. We can see that the highest number of posts were created during the week, with around 15% of posts on each of the weekdays. In contrast, Sunday was the quietest day for popular&nbsp;posts.</p>
<pre><code class="python">rel_df['DayOfWeek'].value_counts()
</code></pre>

<pre><code>Tues     156
Weds     155
Mon      155
Thurs    154
Fri      148
Sun      121
Sat      111
Name: DayOfWeek, dtype: int64
</code></pre>
<pre><code class="python">100 * rel_df['DayOfWeek'].value_counts() / rel_df['DayOfWeek'].notnull().sum()
</code></pre>

<pre><code>Tues     15.6
Weds     15.5
Mon      15.5
Thurs    15.4
Fri      14.8
Sun      12.1
Sat      11.1
Name: DayOfWeek, dtype: float64
</code></pre>
<p>We now have a cleaned dataset and have inspected each of the variables (although be aware I took some shortcuts with my screening and didn&#8217;t inspect things like normality). We are now ready to run some analyses next&nbsp;week.</p>
<p>For those who have followed this tutorial so far, or have been reading my blog more generally, a huge thank you! Today marks just past 3 months of blogging for me, and it has been wonderful to have an excuse to constantly learn new data science skills and to share them with others. I hope my posts have helped you to learn something just as I have learned, and continue to learn from all of the wonderful data science and programming bloggers out there in&nbsp;turn.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>