<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Reading S3 data into a Spark DataFrame using Sagemaker</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Reading S3 data into a Spark DataFrame using Sagemaker written August 10, 2020 in aws,pyspark,sagemaker">
  <meta name="keywords" content="python, data science, aws, sagemaker, s3, pyspark" />

    <link
      rel="canonical"
      href="/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html"
    />

    <link href="/favicon.png" rel="icon" />

      <link
      href="/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Reading S3 data into a Spark DataFrame using Sagemaker</h1>
        <div class="meta">
          written <time datetime="2020-08-10T00:00:00+02:00">August 10, 2020</time>
          in <span class="categories">
            <a href="/tag/aws.html">aws</a>,            <a href="/tag/pyspark.html">pyspark</a>,            <a href="/tag/sagemaker.html">sagemaker</a>          </span>
        </div>
        <p>I have recently been working my way through Jose Portilla&#8217;s excellent <a href="https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/">Udemy course on PySpark</a>, and of course I wanted to try out some things I learned in the course. I have been transitioning over to <a href="https://aws.amazon.com/sagemaker/"><span class="caps">AWS</span> Sagemaker</a> for a lot of my work, but I haven&#8217;t tried using it with PySpark yet. Unfortunately, setting up my Sagemaker notebook instance to read data from S3 using Spark turned out to be one of <em>those</em> issues in <span class="caps">AWS</span>, where it took 5 hours of wading through the <span class="caps">AWS</span> documentation, the PySpark documentation and (of course) StackOverflow before I was able to make it work. Given how painful this was to solve and how confusing the documentation on this generally is, I figured I would write a blog post to hopefully help anyone who gets similarly&nbsp;stuck.</p>
<h2>Setting up S3 and <span class="caps">AWS</span>&nbsp;correctly</h2>
<p>The first thing that you need to ensure is that Sagemaker has permission to access S3 and read the data in the first place. The easiest way I&#8217;ve found to do this (as an <span class="caps">AWS</span> beginner) is to set up <span class="caps">IAM</span> role for all of your Sagemaker notebooks, which allows them (among other things) to read data from S3 buckets. <a href="https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html">This guide</a> walks you through all of the steps to do&nbsp;this.</p>
<p>Next, you need to go back over to Sagemaker and create your notebook instance. While you&#8217;re creating it, you&#8217;ll see an option under &#8220;Permissions and encryption&#8221; to set the <span class="caps">IAM</span> role. You should select the role you just created in the step above. As you can see, I called my role <code>AWSGlueServiceSageMakerNotebookRole-Default</code>, as recommended in the tutorial. You can leave all of the other options as their defaults and create your notebook&nbsp;instance.  </p>
<p><img src="/figure/sagemaker-spark-1.png" title="Setting permissions" style="display: block; margin: auto;" /></p>
<p>You now need somewhere to store all of your data. Go over to S3 and create a new bucket. <span class="caps">AWS</span> recommends that you <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html">prefix the name of your bucket with Sagemaker</a>, but I don&#8217;t think this is necessary for Sagemaker to be able to recognise the bucket. You can now upload your data into the&nbsp;bucket.  </p>
<p>Next, you will need to retrieve your <code>AWSAccessKeyId</code> and <code>AWSSecretKey</code>, which will be needed for PySpark to read in the data. <a href="https://supsystic.com/documentation/id-secret-access-key-amazon-s3/">This guide</a> steps you through how to generate and retrieve&nbsp;these.</p>
<p>Finally, go back to your notebook instance in Sagemaker and open up JupyterLab. Scroll down to the bottom of the Launcher screen to the &#8220;Other&#8221; applications, and open up Terminal. As per <a href="https://sagemaker-pyspark.readthedocs.io/en/latest/">this guide</a>, we need to check our <code>config</code> file is set to the right <span class="caps">AWS</span> region and also put our <code>AWSAccessKeyId</code> and <code>AWSSecretKey</code> in the <code>credentials</code> file. To get started, navigate to <code>~/.aws</code> and check the&nbsp;contents:</p>
<div class="highlight"><pre><span></span><span class="err">!</span><span class="n">cd</span> <span class="o">~/.</span><span class="n">aws</span>
<span class="err">!</span><span class="n">ls</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="err">config  credentials</span>
</pre></div>


<p>Open the config file, and check that the region matches the one you&#8217;ve set your Sagemaker notebook up in. For example, my notebook is in the Frankfurt region, so my config file looks like&nbsp;this:</p>
<div class="highlight"><pre><span></span><span class="err">!</span><span class="n">head</span> <span class="o">~/.</span><span class="n">aws</span><span class="o">/</span><span class="n">config</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[default]</span>
<span class="na">region</span> <span class="o">=</span> <span class="s">eu-central-1</span>
</pre></div>


<p>Next, if you don&#8217;t have a credentials file, you&#8217;ll need to create one. Inside, you need to paste your <code>AWSAccessKeyId</code> and <code>AWSSecretKey</code> in the following&nbsp;format:</p>
<div class="highlight"><pre><span></span><span class="p">[</span><span class="n">default</span><span class="p">]</span>
<span class="n">aws_access_key_id</span> <span class="o">=</span> <span class="n">YOUR_KEY</span>
<span class="n">aws_secret_access_key</span> <span class="o">=</span> <span class="n">YOUR_KEY</span>
</pre></div>


<h2>Configuring <code>sagemaker_pyspark</code></h2>
<p>We&#8217;ve now finished all of the preparatory steps, and you can now create a new <code>python_conda3</code> notebook. Once we have this notebook, we need to configure our <code>SparkSession</code> correctly.</p>
<p>When I initially started trying to read my file into a Spark DataFrame, I kept getting the following error: <code>Py4JJavaError: An error occurred while calling o65.csv. : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found</code>. I was missing a step where I needed to <a href="https://sagemaker-pyspark.readthedocs.io/en/latest/">load the Sagemaker <span class="caps">JAR</span> files</a> in order for Spark to work properly. You can see this in the code below, where I used <code>SparkConf</code> to do&nbsp;this.</p>
<p>Finally, we need to also make the <code>AWSAccessKeyId</code> and <code>AWSSecretKey</code> visible to the <code>SparkSession</code>. We can use a package called <code>botocore</code> to <a href="https://stackoverflow.com/questions/50499894/how-should-i-load-file-on-s3-using-spark">access the credentials</a> from the <code>~/.aws/credentials</code> file we created earlier. You can see that instead of us needing to pass the credentials directly, the <code>botocore</code> session pulls them out of our credentials file and stores them for us. We&#8217;ve also passed the <code>SparkConf</code> that we created as a config in the <code>SparkSession</code> builder as&nbsp;well.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="nn">sagemaker_pyspark</span>
<span class="kn">import</span> <span class="nn">botocore.session</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">botocore</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span>
<span class="n">credentials</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_credentials</span><span class="p">()</span>

<span class="n">conf</span> <span class="o">=</span> <span class="p">(</span><span class="n">SparkConf</span><span class="p">()</span>
        <span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.driver.extraClassPath&quot;</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sagemaker_pyspark</span><span class="o">.</span><span class="n">classpath_jars</span><span class="p">())))</span>

<span class="n">spark</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">SparkSession</span>
    <span class="o">.</span><span class="n">builder</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;fs.s3a.access.key&#39;</span><span class="p">,</span> <span class="n">credentials</span><span class="o">.</span><span class="n">access_key</span><span class="p">)</span>
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;fs.s3a.secret.key&#39;</span><span class="p">,</span> <span class="n">credentials</span><span class="o">.</span><span class="n">secret_key</span><span class="p">)</span>
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;schema_test&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>


<p>Alternatively, if you&#8217;re having problems with <code>botocore</code> reading in your credentials, you can also paste in your <code>AWSAccessKeyId</code> and <code>AWSSecretKey</code> directly as strings. This is obviously a bit less secure, so make sure you delete them from your notebook before sharing it with&nbsp;anyone!</p>
<div class="highlight"><pre><span></span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
        <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;schema_test&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;fs.s3a.access.key&#39;</span><span class="p">,</span> <span class="n">YOUR_KEY</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;fs.s3a.secret.key&#39;</span><span class="p">,</span> <span class="n">YOUR_KEY</span><span class="p">)</span>\
        <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>


<p>And that&#8217;s it, we&#8217;re done! We can finally load in our data from S3 into a Spark DataFrame, as&nbsp;below.</p>
<div class="highlight"><pre><span></span><span class="n">bucket</span> <span class="o">=</span> <span class="s2">&quot;sagemaker-pyspark&quot;</span>
<span class="n">data_key</span> <span class="o">=</span> <span class="s2">&quot;train_sample.csv&quot;</span>
<span class="n">data_location</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;s3a://</span><span class="si">{bucket}</span><span class="s2">/</span><span class="si">{data_key}</span><span class="s2">&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">data_location</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="n">inferSchema</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TRIP_ID</th>
      <th>CALL_TYPE</th>
      <th>ORIGIN_CALL</th>
      <th>ORIGIN_STAND</th>
      <th>TAXI_ID</th>
      <th><span class="caps">TIMESTAMP</span></th>
      <th>DAY_TYPE</th>
      <th>MISSING_DATA</th>
      <th><span class="caps">POLYLINE</span></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1372636858620000589</td>
      <td>C</td>
      <td>None</td>
      <td>NaN</td>
      <td>20000589</td>
      <td>1372636858</td>
      <td>A</td>
      <td>False</td>
      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[&#8230;</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1372637303620000596</td>
      <td>B</td>
      <td>None</td>
      <td>7.0</td>
      <td>20000596</td>
      <td>1372637303</td>
      <td>A</td>
      <td>False</td>
      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[&#8230;</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1372636951620000320</td>
      <td>C</td>
      <td>None</td>
      <td>NaN</td>
      <td>20000320</td>
      <td>1372636951</td>
      <td>A</td>
      <td>False</td>
      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-&#8230;</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1372636854620000520</td>
      <td>C</td>
      <td>None</td>
      <td>NaN</td>
      <td>20000520</td>
      <td>1372636854</td>
      <td>A</td>
      <td>False</td>
      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[&#8230;</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1372637091620000337</td>
      <td>C</td>
      <td>None</td>
      <td>NaN</td>
      <td>20000337</td>
      <td>1372637091</td>
      <td>A</td>
      <td>False</td>
      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-&#8230;</td>
    </tr>
  </tbody>
</table>
</div>

<p>I hope this guide was useful and helps you troubleshoot any of the problems you might be having getting PySpark to work with Sagemaker, and getting it to read in your data from&nbsp;S3.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

   </body>
</html>