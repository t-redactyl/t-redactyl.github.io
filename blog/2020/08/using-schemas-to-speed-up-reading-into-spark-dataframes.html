<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Using schemas to speed up reading into Spark DataFrames</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Using schemas to speed up reading into Spark DataFrames written August 24, 2020 in pyspark">
  <meta name="keywords" content="python, data science, aws, sagemaker, s3, pyspark" />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2020/08/using-schemas-to-speed-up-reading-into-spark-dataframes.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Using schemas to speed up reading into Spark DataFrames</h1>
        <div class="meta">
          written <time datetime="2020-08-24T00:00:00+02:00">August 24, 2020</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/pyspark.html">pyspark</a>          </span>
        </div>
        <p>While Spark is the best thing since sliced bread for dealing with big data, I definitely realise I have a lot to learn before I can use it to its full potential. One trick I recently discovered was using explicit schemas to speed up how fast PySpark can read a <span class="caps">CSV</span> into a&nbsp;DataFrame.</p>
<p>When using <code>spark.read_csv</code> to read in a <span class="caps">CSV</span> in PySpark, the most straightforward way is to set the <code>inferSchema</code> argument to <code>True</code>. This means that PySpark will attempt to check the data in order to work out what type of data each column&nbsp;is. </p>
<p>The problem with this operation is that it&#8217;s quite memory intensive, especially for large datasets, as Spark needs to look at a sufficient amount of data in order to correctly infer the type. Imagine that you have a column with integers in the first 1000 rows, but then a string in the 1001th row. If PySpark had inferred this column was an <code>IntegerType</code> based on the top few rows, then we would end up with missing values for each of the rows containing a string. As such, PySpark either needs to scan the whole dataset, or randomly sample enough rows to infer the&nbsp;type.</p>
<p>One way we can get around this is by determining the type of the data beforehand and passing this information to PySpark using an schema. The syntax for this is pretty straightforward. Each column&#8217;s name and type is defined using the <code>StructField</code> method from <code>pyspark.sql</code>:</p>
<pre><code class="python">StructField(&quot;trip_id&quot;, StringType(), True)
</code></pre>

<p>The three arguments that <code>StructField</code> takes are the name you&#8217;d like to give the column, the column&#8217;s data type, and whether the field can contain null values (as a boolean). The available column data types are also in <code>pyspark.sql</code>, and cover a wide type of possible data types, from string, float and integer to boolean and datetime. A <code>StructField</code> is created for each column, and these are passed as a list to <code>pyspark.sql</code><span class="quo">&#8216;</span>s <code>StructType</code>. This schema can then be passed to <code>spark.read.csv</code><span class="quo">&#8216;</span>s <code>schema</code> argument.</p>
<pre><code class="python">StructType(
    [StructField(&quot;trip_id&quot;, StringType(), True),
    StructField(&quot;call_type&quot;, StringType(), True)]
)
</code></pre>

<p>In order to test how much faster it is to use a schema, I will be using the <a href="https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015">Taxi Service Trajectory</a> dataset from the <span class="caps">UCI</span> Machine Learning Repository. This dataset has 9 columns and around 1.7 million&nbsp;rows.</p>
<p>We&#8217;ll first set up our <code>SparkSession</code> to be able to access our data on&nbsp;S3:</p>
<pre><code class="python">from pyspark import SparkConf
from pyspark.sql import SparkSession
import sagemaker_pyspark
import botocore.session

from time import time
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType

session = botocore.session.get_session()
credentials = session.get_credentials()

conf = (SparkConf()
        .set(&quot;spark.driver.extraClassPath&quot;, &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())))

spark = (
    SparkSession
    .builder
    .config(conf=conf) \
    .config('fs.s3a.access.key', credentials.access_key)
    .config('fs.s3a.secret.key', credentials.secret_key)
    .appName(&quot;schema_test&quot;)
    .getOrCreate()
)
</code></pre>

<p>We&#8217;ll also set up our data&nbsp;path:</p>
<pre><code class="python">bucket = &quot;sagemaker-pyspark&quot;
data_key = &quot;train.csv&quot;
data_location = f&quot;s3a://{bucket}/{data_key}&quot;
</code></pre>

<p>Let&#8217;s first time how long it takes to read in these data when PySpark has to infer the&nbsp;schema:</p>
<pre><code class="python">t1 = time()

data_inferred = spark.read.csv(data_location, header = 'True', inferSchema = True)

t2 = time()
print('Completed in %s sec.' % (str(t2 - t1)))
</code></pre>

<pre><code>Completed in 28.1013503074646 sec.
</code></pre>
<p>We&#8217;ll now compare how long it takes when we explicitly tell PySpark what the data schema&nbsp;is:</p>
<pre><code class="python">t1 = time()

schema = StructType(
    [StructField(&quot;trip_id&quot;, StringType(), True),
    StructField(&quot;call_type&quot;, StringType(), True),
    StructField(&quot;origin_call&quot;, IntegerType(), True),
    StructField(&quot;origin_stand&quot;, IntegerType(), True),
    StructField(&quot;taxi_id&quot;, LongType(), True),
    StructField(&quot;timestamp&quot;, LongType(), True),
    StructField(&quot;day_type&quot;, StringType(), True),
    StructField(&quot;missing_data&quot;, BooleanType(), True),
    StructField(&quot;polyline&quot;, StringType(), True)]
)
data_schema = spark.read.csv(data_location, header = 'False', schema = schema)

t2 = time()
print('Completed in %s sec.' % (str(t2 - t1)))
</code></pre>

<pre><code>Completed in 2.506075620651245 sec.
</code></pre>
<p>The data are read in around 10 times faster when we give PySpark the schema rather than asking it to infer it! Obviously this is a more practical step when you have data with fewer variables, but when reading truly large data, it is an easy way to save some processing&nbsp;time.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>