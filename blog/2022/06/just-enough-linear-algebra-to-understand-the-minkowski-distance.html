<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Just enough linear algebra to understand the Minkowski distance</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Just enough linear algebra to understand the Minkowski distance written June 07, 2022 in python,machine learning">
  <meta name="keywords" content="numpy, python, minkowski distance, euclidean distance, manhattan distance," />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2022/06/just-enough-linear-algebra-to-understand-the-minkowski-distance.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Just enough linear algebra to understand the Minkowski distance</h1>
        <div class="meta">
          written <time datetime="2022-06-07T00:00:00+02:00">June 07, 2022</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/python.html">python</a>,            <a href="https://t-redactyl.github.io/tag/machine-learning.html">machine learning</a>          </span>
        </div>
        <p>If you had to invent a machine learning algorithm from scratch, what would be some of the ways you&#8217;d find patterns in your data? One idea that you might have come up with is to assume that data points that are &#8220;close&#8221; to each other are similar, and those that are &#8220;distant&#8221; are dissimilar. You would not be alone in this thinking: some of the most important models and metrics in machine learning rely on finding the distances between data points, including <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-nearest neighbours classification</a>, <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a> and loss functions such as <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a>.</p>
<p>The question then comes: how do we define what we mean by close or distant? The first thing we need to understand is that all of our data points sit inside of what is called a <a href="https://en.wikipedia.org/wiki/Vector_space">vector space</a>. Don&#8217;t be deterred by the terminology! Once we break it down and explain it you&#8217;ll see this is quite simple to&nbsp;understand.</p>
<p>For this blog post, we&#8217;re going to use the popular <a href="https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset#">dry bean dataset</a>, which describes the characteristics of 13,611 images of dried beans across 7 different types. There are 16 features describing the beans, such as their area, aspect ratio and&nbsp;roundness.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">beans</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s2">&quot;data/Dry_Bean_Dataset.xlsx&quot;</span><span class="p">)</span>
</code></pre></div>

<h2>Vector&nbsp;spaces</h2>
<p>The easiest way to understand vector spaces is to start in two dimensions. Let&#8217;s say we take two of our features: major and minor axis length, which are just the length and width of the bean respectively. If we plot a small number of our observations using these features, we can see that beans of the same type tend to be close to each other, and beans that are of different types tend to be far&nbsp;apart.</p>
<p>This is a 2-dimensional vector space, with each of our two features representing the x- and y-axes respectively, and the distance of data points (in this case, beans) to each other in this vector space is an indication of their similarity. Each bean in this dataset has a coordinate in this vector space represented by their values of the features. For example, the coordinate of the Bombay bean in this vector space below is <span class="math">\((653.68, 392.26)\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">plotnine</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">plotnine</span> <span class="kn">import</span> <span class="n">ggplot</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">beans</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)[[</span><span class="s2">&quot;MajorAxisLength&quot;</span><span class="p">,</span> <span class="s2">&quot;MinorAxisLength&quot;</span><span class="p">,</span> <span class="s2">&quot;Class&quot;</span><span class="p">]],</span>
           <span class="n">aes</span><span class="p">(</span><span class="s2">&quot;MajorAxisLength&quot;</span><span class="p">,</span> <span class="s2">&quot;MinorAxisLength&quot;</span><span class="p">,</span> <span class="n">colour</span> <span class="o">=</span> <span class="s2">&quot;Class&quot;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Major axis length&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Minor axis length&quot;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>

<p><img src="/figure/minkowski_1.png" title="2 dimensional vector space" style="display: block; margin: auto;" /></p>
<p>We can extend this idea by seeing what happens if we plot the data against a third feature, <code>roundness</code>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">beans_sample</span> <span class="o">=</span> <span class="n">beans</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)[[</span><span class="s2">&quot;MajorAxisLength&quot;</span><span class="p">,</span> <span class="s2">&quot;MinorAxisLength&quot;</span><span class="p">,</span> <span class="s2">&quot;roundness&quot;</span><span class="p">,</span> <span class="s2">&quot;Class&quot;</span><span class="p">]]</span>
<span class="n">col</span><span class="o">=</span><span class="n">beans_sample</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;DERMASON&#39;</span><span class="p">:</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;CALI&#39;</span><span class="p">:</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;BOMBAY&#39;</span><span class="p">:</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s2">&quot;SEKER&quot;</span><span class="p">:</span> <span class="s2">&quot;purple&quot;</span><span class="p">})</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span> <span class="o">=</span> <span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Major axis length&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Minor axis length&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;Roundness&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">beans_sample</span><span class="p">[</span><span class="s2">&quot;MajorAxisLength&quot;</span><span class="p">],</span> <span class="n">beans_sample</span><span class="p">[</span><span class="s2">&quot;MinorAxisLength&quot;</span><span class="p">],</span> <span class="n">beans_sample</span><span class="p">[</span><span class="s2">&quot;roundness&quot;</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="n">col</span><span class="p">)</span>
</code></pre></div>

<p><img src="/figure/minkowski_2.png" title="3 dimensional vector space" style="display: block; margin: auto;" /></p>
<p>You&#8217;ve probably already guessed that this graph is visualising a 3-dimensional vector space. As with the 2-dimensional vector space, the three features form the x-, y- and z-axes of this feature space, and the coordinates of the beans in this space are represented by their values of each of the features. In this larger feature space, the coordinate of the Bombay bean is now <span class="math">\((653.68, 392.26, 0.85)\)</span>. You can also see that, as with the 2-dimensional vector space, the distance between the points indicates their similarity, with beans that are of the same type grouping together, and those that are of different types falling far apart. We can keep extending this idea logically up to as many dimensions as we have features, but past the point of 3-dimensions we can no longer visualise&nbsp;it.</p>
<h2>The Minkowski distance&nbsp;metrics</h2>
<p>The question that has likely been on your mind during the previous section is: well, <em>how</em> do we actually measure these distances? It turns out there are quite a lot of different metrics, but today we&#8217;ll be discussing two of the most commonly used, the <a href="https://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan</a> and <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean</a>&nbsp;distances.</p>
<p>In 2-dimensional vector spaces, these distance metrics are very intuitive to understand. For the Manhattan distance, imagine that you were forced to only travel in the direction of the x- and y-axes. In the example below, we have a point at <span class="math">\((2, 1)\)</span> and another at <span class="math">\((4, 3)\)</span>. In order to travel from <span class="math">\((2, 1)\)</span> to <span class="math">\((4, 3)\)</span> using the Manhattan distance, we would need to first travel 2 units along the x-axis, and then another 2 units along the y-axis, giving us a total Manhattan distance of&nbsp;4.</p>
<p><img src="/figure/minkowski_3.png" title="Manhattan distance" style="display: block; margin: auto;" /></p>
<p>The Euclidean distance, by contrast, takes the &#8220;direct&#8221; path from one point to another. If you look at the diagram below, you can see that the path travelled for the Euclidean distance and the paths travelled for the Manhattan distance form a triangle, with the Euclidean distance as the&nbsp;hypotenuse.</p>
<p><img src="/figure/minkowski_4.png" title="Euclidean distance" style="display: block; margin: auto;" /></p>
<p>This gives us a clue as to how to calculate the Euclidean distance: we can use the <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean theorem</a>, treating the Euclidean distance path as our hypotenuse and the Manhattan distance path as our other two triangle sides. If we do this below, we&nbsp;get:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">calculate_hypotenuse</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                         <span class="n">b</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Given the two shorter triangle sides, uses the Pythagorean theorem to calculate the length of the hypotenuse.&quot;&quot;&quot;</span>
    <span class="n">c_sq</span> <span class="o">=</span> <span class="n">a</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">c_sq</span><span class="p">)</span>

<span class="n">calculate_hypotenuse</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">2.8284271247461903</span>
</code></pre></div>

<p>And voilà, the Euclidean distance between the points is&nbsp;2.83.</p>
<p>As with vector spaces, we can generalise the Manhattan and Euclidean distances to whatever number of dimensions we want. But this method we&#8217;ve used above of calculating distances is very clunky, and it would be better to have a formula. Luckily, we not only have a formula, but it is the <em>same</em> formula for both distance metrics: the <a href="https://en.wikipedia.org/wiki/Minkowski_distance">Minkowski distance metric</a>, shown&nbsp;below:</p>
<div class="math">$$
D(X, Y) = \left(\sum_{i=1}^{n-1} | x_i - y_i |^p \right)^\frac{1}{p}
$$</div>
<p>Let&#8217;s break this down. The first thing we need to do before we dive into the formula is start thinking of the coordinates of our points, e.g., <span class="math">\((2, 1)\)</span>, as <em>vectors</em>. These are essentially an ordered list that gives the position of a point in n-dimensions, with the number of elements of the vector corresponding to the number of dimensions. We&#8217;ve seen this already with our explanation of 2- and 3-dimensional vector spaces, where the coordinates of the 2-dimensional space represented the bean&#8217;s value for each of the major and minor axes, and in the 3-dimensional space, the bean&#8217;s value for each of the major and minor axes and the roundness. For our purposes, vectors and coordinates are&nbsp;interchangeable.</p>
<p>The Minkowski distance formula is first telling us that we have a pair of vectors, <span class="math">\(X\)</span> and <span class="math">\(Y\)</span>, and we want to find the distance <span class="math">\(D\)</span> between them. Each vector has <span class="math">\(i\)</span> elements, so for a pair of vectors in a 4-dimensional vector space, we would have 4 elements in each. We then pair up the corresponding elements in each vector, take the absolute difference of each, and then raise that to the power of a number, <span class="math">\(p\)</span> (we&#8217;ll come back to this value). Finally, we sum all of these differences, and then raise this sum to the power of <span class="math">\(\frac{1}{p}\)</span>.</p>
<p>To make this really concrete, let&#8217;s work through an example. We need a few different values for this: the vectors <span class="math">\(X\)</span> and <span class="math">\(Y\)</span>; the value of <span class="math">\(i\)</span> and the value of <span class="math">\(p\)</span>.</p>
<p>Before we get to the example, let&#8217;s talk about <span class="math">\(p\)</span>. This is the value that allows us to switch between calculating the Manhattan and Euclidean distances, with <span class="math">\(p = 1\)</span> giving us the Manhattan distance and <span class="math">\(p = 2\)</span> giving us the Euclidean distance. We&#8217;ll have a look at how this works as we go through the example. Let&#8217;s get&nbsp;started.</p>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">i</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">diff_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">diff_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">diff_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">diff_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</code></pre></div>

<p>We have two vectors here, <span class="math">\(X = (5, 7, 3, 9)\)</span> and <span class="math">\(Y = (1, 6, 2, 4)\)</span>, which we&#8217;ve represented using Python lists. As they have 4 elements each, they are in a 4-dimensional vector space and therefore <span class="math">\(i = 4\)</span>. We then take the absolute difference of each paired element in these vectors. However, let&#8217;s see what happens when we try to raise one of these differences to <span class="math">\(p\)</span> when <span class="math">\(p = 1\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="n">diff_1</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">4</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">diff_1</span><span class="o">**</span><span class="n">p</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">4</span>
</code></pre></div>

<p>It&#8217;s the same value! This is because any value raised to the power of 1 returns the same value, so if we raised each of our absolute differences to 1, they would remain unchanged. We&#8217;ll therefore leave out this step and proceed straight to adding all of these&nbsp;differences.</p>
<div class="highlight"><pre><span></span><code><span class="n">manhattan_distance</span> <span class="o">=</span> <span class="n">diff_1</span> <span class="o">+</span> <span class="n">diff_2</span> <span class="o">+</span> <span class="n">diff_3</span> <span class="o">+</span> <span class="n">diff_4</span>
<span class="n">manhattan_distance</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">11</span>
</code></pre></div>

<p>You can see we&#8217;ve also left off the final step of raising this sum of differences to <span class="math">\(\frac{1}{p}\)</span>. This is because <span class="math">\(\frac{1}{1} = 1\)</span>, so again, we have no need to apply this step. Therefore, we have arrived at a final Manhattan distance of&nbsp;11.</p>
<p>I hope you can see the parallels with what we did in our initial example on the Manhattan distance in a 2-dimensional vector space. What we&#8217;ve just done is travelled the distance along the x-axis, y-axis, z-axis and the fourth w-axis between the two points, and then simply added them together. Nothing more fancy than&nbsp;that.</p>
<p>Let&#8217;s now work through our example using <span class="math">\(p = 2\)</span> for the Euclidean&nbsp;distance.</p>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">i</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">diff_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="n">p</span>
<span class="n">diff_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="n">p</span>
<span class="n">diff_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">**</span><span class="n">p</span>
<span class="n">diff_4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">Y</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">**</span><span class="n">p</span>

<span class="n">euclidean_distance_sq</span> <span class="o">=</span> <span class="n">diff_1</span> <span class="o">+</span> <span class="n">diff_2</span> <span class="o">+</span> <span class="n">diff_3</span> <span class="o">+</span> <span class="n">diff_4</span>
<span class="n">euclidean_distance</span> <span class="o">=</span> <span class="n">euclidean_distance_sq</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">p</span><span class="p">)</span>
<span class="n">euclidean_distance</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">6.557438524302</span>
</code></pre></div>

<p>You can see here that we&#8217;ve taken the differences between each paired element, taken their absolute value, and then squared them. We&#8217;ve then added them together to get the <em>squared</em> Euclidean distance, and then finally raised them to the power of <span class="math">\(\frac{1}{2}\)</span> (in other words, taken the square root) to get the final Euclidean distance. Again, you can see the parallels with our first example of calculating the Euclidean distance. In that example, we used the Pythagorean theorem, but we are essentially doing the same thing here: getting the sides of a 4-dimensional triangle by taking the elementwise difference between the vectors, squaring them, adding them and then taking the square&nbsp;root.</p>
<p>We can double-check our results by using <code>sklearn</code><span class="quo">&#8216;</span>s <code>DistanceMetric</code> method.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">DistanceMetric</span>

<span class="n">manhattan</span> <span class="o">=</span> <span class="n">DistanceMetric</span><span class="o">.</span><span class="n">get_metric</span><span class="p">(</span><span class="s2">&quot;manhattan&quot;</span><span class="p">)</span>
<span class="n">manhattan</span><span class="o">.</span><span class="n">pairwise</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[ 0., 11.],</span>
<span class="err">       [11.,  0.]])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">euclidean</span> <span class="o">=</span> <span class="n">DistanceMetric</span><span class="o">.</span><span class="n">get_metric</span><span class="p">(</span><span class="s2">&quot;euclidean&quot;</span><span class="p">)</span>
<span class="n">euclidean</span><span class="o">.</span><span class="n">pairwise</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">array([[0.        , 6.55743852],</span>
<span class="err">       [6.55743852, 0.        ]])</span>
</code></pre></div>

<p>As you can see, we&#8217;ve arrived at the exact same result by hand as <code>sklearn</code> does under the&nbsp;hood.</p>
<p>To finish, let&#8217;s apply what we&#8217;ve learnt to our original sample of data from the dry beans dataset. From our visualisations, we would expect that beans that belong to the same type would have lower distances to each other than beans from different types. Let&#8217;s assess this using the three features we visualised: <code>MajorAxisLength</code>, <code>MinorAxisLength</code> and <code>roundness</code>. We&#8217;ll take two example beans from <code>DERMASON</code> and two from <code>CALI</code> and compare their&nbsp;distances.</p>
<div class="highlight"><pre><span></span><code><span class="n">dermason</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">beans_sample</span><span class="p">[</span><span class="n">beans_sample</span><span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;DERMASON&quot;</span><span class="p">]</span>
        <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">])</span>
        <span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">cali</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">beans_sample</span><span class="p">[</span><span class="n">beans_sample</span><span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;CALI&quot;</span><span class="p">]</span>
        <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">])</span>
        <span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div>

<p>First, we calculate the pairwise Manhattan&nbsp;distances.</p>
<div class="highlight"><pre><span></span><code><span class="n">beans_manhattan</span> <span class="o">=</span> <span class="n">manhattan</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dermason</span><span class="p">,</span> <span class="n">cali</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Manhattan distance between the two dermason beans is: </span><span class="si">{</span><span class="n">beans_manhattan</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Manhattan distance between the two cali beans is: </span><span class="si">{</span><span class="n">beans_manhattan</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Manhattan distance between a dermason and a cali bean is: </span><span class="si">{</span><span class="n">beans_manhattan</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">The Manhattan distance between the two dermason beans is:  21.73</span>
<span class="err">The Manhattan distance between the two cali beans is:  39.98</span>
<span class="err">The Manhattan distance between a dermason and a cali bean is:  283.29</span>
</code></pre></div>

<p>And then the Euclidean&nbsp;distances.</p>
<div class="highlight"><pre><span></span><code><span class="n">beans_euclidean</span> <span class="o">=</span> <span class="n">euclidean</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dermason</span><span class="p">,</span> <span class="n">cali</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Euclidean distance between the two dermason beans is: </span><span class="si">{</span><span class="n">beans_euclidean</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Euclidean distance between the two cali beans is: </span><span class="si">{</span><span class="n">beans_euclidean</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The Euclidean distance between a dermason and a cali bean is: </span><span class="si">{</span><span class="n">beans_euclidean</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">The Euclidean distance between the two dermason beans is:  15.65</span>
<span class="err">The Euclidean distance between the two cali beans is:  28.27</span>
<span class="err">The Euclidean distance between a dermason and a cali bean is:  215.89</span>
</code></pre></div>

<p>We can see that, as in the graph, the distances between beans of the same type is much smaller than those of different types, for both distance&nbsp;metrics.</p>
<p>That&#8217;s it for this post: next time, we&#8217;ll discuss how to implement the Minkowski distance formula in a few different ways. As this is a very expensive operation on large datasets, we&#8217;ll be breaking down how to iteratively get better performance using a couple of tricks in&nbsp;Numpy.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>