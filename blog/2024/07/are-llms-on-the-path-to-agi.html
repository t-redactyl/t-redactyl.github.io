<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Are LLMs on the path to AGI?</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Are LLMs on the path to AGI? written July 27, 2024 in llms,machine learning">
  <meta name="keywords" content="psychology, intelligence, agi, artificial general intelligence,, artificial superintelligence, llms, large languages models, gpt-1, gpt-2, gpt-3.5, gpt-4o, language" />

    <link
      rel="canonical"
      href="/blog/2024/07/are-llms-on-the-path-to-agi.html"
    />

    <link href="/favicon.png" rel="icon" />

      <link
      href="/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Are LLMs on the path to AGI?</h1>
        <div class="meta">
          written <time datetime="2024-07-27T00:00:00+02:00">July 27, 2024</time>
          in <span class="categories">
            <a href="/tag/llms.html">llms</a>,            <a href="/tag/machine-learning.html">machine learning</a>          </span>
        </div>
        <p>The claims of <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial general intelligence, or <span class="caps">AGI</span></a>, have been some of the hottest and most emotionally charged discussions about LLMs. In addition, these claims probably have the most intellectual weight behind them of all of the claims about <span class="caps">LLM</span> abilities. For example, in March 2023, Microsoft research released a paper titled <a href="https://arxiv.org/pdf/2303.12712">&#8220;Sparks of Artificial General Intelligence&#8221;</a>, where they claimed that a series of experiments on <span class="caps">GPT</span>-4 showed that this model was showing at least some signs of <span class="caps">AGI</span>. Later that year, researchers <a href="https://en.wikipedia.org/wiki/Blaise_Ag%C3%BCera_y_Arcas">Blaise Agüera y Arcas</a> (a <span class="caps">AI</span> research fellow at Google) and <a href="https://de.wikipedia.org/wiki/Peter_Norvig">Peter Norvig</a> (a Distinguished Education Fellow at the Stanford Institute for Human-Centered <span class="caps">AI</span>) <a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/">published a piece</a> stating that LLMs had already crossed the threshold for <span class="caps">AGI</span>. </p>
<p>So with these academic heavy hitters, plus many others, throwing their weight behind the idea of LLMs showing <span class="caps">AGI</span>, how can we be sure that these models haven’t developed intelligence&nbsp;yet?</p>
<p>This and the other two blog posts in this series are based on a <a href="https://2024.pycon.it/en/keynotes/mirror-mirror-llms-and-the-illusion-of-humanity">keynote I delivered at PyCon Italia</a> this year, as well as a talk at <a href="https://ndcoslo.com/agenda/mirror-mirror-llms-and-the-illusion-of-humanity-0ufl/0ewbqkadnxm"><span class="caps">NDC</span> Oslo</a>, where I debunk some of the more outrageous claims about LLMs demonstrating human-level traits and behaviours. If you also want to give the whole talk a watch, you can see it below. You can also read the previous posts in this series, the <a href="/blog/2024/06/can-llms-use-language-at-a-human-like-level.html">first</a> where I discuss why LLMs are not using language in the same we do as humans, and the <a href="/blog/2024/07/could-llms-be-sentient.html">second</a> where I explain why LLMs are likely not&nbsp;sentient.</p>
<iframe width="800" height="448"
src="https://www.youtube.com/embed/2jo2__9uulA?si=VIVhLY0DSErNxFKj&amp;start=1368">
</iframe>

<p>The work in this blogpost is based on the following three excellent articles: <a href="https://arxiv.org/pdf/1911.01547">&#8220;On the Measure of Intelligence&#8221;</a> by François Chollet, <a href="https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks">&#8220;<span class="caps">GPT</span>-4 and professional benchmarks: the wrong answer to the wrong question&#8221;</a> by Sayash Kapoor and Arvind Narayanan on their excellent blog &#8220;<span class="caps">AI</span> Snake Oil&#8221; (full of many such good reads), and <a href="https://arxiv.org/pdf/2311.02462">&#8220;Levels of <span class="caps">AGI</span> for Operationalizing Progress on the Path to <span class="caps">AGI</span>&#8221;</a> by a team at&nbsp;DeepMind.</p>
<h2>Haven&#8217;t we heard this story&nbsp;before?</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">May of 1997</a>, the then world chess champion <a href="https://en.wikipedia.org/wiki/Garry_Kasparov">Garry Kasparov</a> was facing off against <a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)">Deep Blue</a>, <span class="caps">IBM</span>’s chess-playing <span class="caps">AI</span>. This was their second match, Kasparov having won the first one in February 1996. In the second game of the match, Deep Blue made an unexpected move, which rattled Kasparov and made him believe the algorithm was far more powerful than it actually was. Thrown off his game, he ended up losing the second game to Deep Blue, then drew every game until finally losing the sixth, losing the whole match in the process. The press went wild, speculating that if we could conquer chess with an artificial system, then surely <span class="caps">AGI</span> was around the corner. However, it&#8217;s been almost 30 years, and you can see how those predictions panned&nbsp;out. </p>
<p><img src="/figure/deep-blue-kasparov-1997.png" title="AGI is here!" style="display: block; margin: auto;" /></p>
<p>In fact, since we started developing advanced machines, these predictions that we’re on the cusp of developing an artificial intelligence have been regularly made. As far back as 1863, right at the start of the second industrial revolution, author Samuel Butler stated that the <a href="https://en.wikipedia.org/wiki/Darwin_among_the_Machines">development and ascendency of an artificial general intelligence was inevitable</a>,&nbsp;stating:</p>
<blockquote>
<p>The upshot is simply a question of time, but that time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment&nbsp;question.</p>
</blockquote>
<p>And mathematician Claude Shannon, interviewed only a few years after the development of the first neural net, predicted that <a href="https://blog.datagran.io/posts/why-ai-is-harder-than-we-think-4-fallacies-researches-face"><span class="caps">AGI</span> was only a few years away</a>:</p>
<blockquote>
<p>I confidently expect that within a matter of ten or fifteen years, something will emerge from the laboratories which is not too far from the robot of science fiction&nbsp;fame.</p>
</blockquote>
<p>So as you can see, the current speculation around whether we’ve created <span class="caps">AGI</span> or are on this path is part of a long history. However, just because we’ve had false starts before, doesn’t mean we can just hand wave away the possibility that LLMs are an <span class="caps">AGI</span>. Instead, let’s have a closer look at the&nbsp;evidence.</p>
<h2>Skill-based assessments of intelligence are&nbsp;misleading</h2>
<p>The problem with trying to assess intelligence through whether a system does well at some complex task is that it <a href="https://arxiv.org/pdf/1911.01547">confuses the output of an <span class="caps">AI</span> system with the mechanism the model took to get there</a>. In humans, we can use skill-based assessments of intelligence, because we know that in humans, the ability to demonstrate significant skill in an area is reflective of an underlying raw general&nbsp;ability.</p>
<p><span class="caps">AI</span> models - or, let&#8217;s call them what they are - machine learning models, don’t work this way. They will try to optimise for their training goals, and if they can take shortcuts to get to good performance on these goals, they will. The mistake is thinking that the pathway required to learn some skill requires the development of underlying&nbsp;intelligence.</p>
<p>This illusion of skill-based intelligence can be seen on websites like <a href="https://www.kaggle.com/">Kaggle</a>. On this site, people compete to create a machine learning algorithm that can get the best performance on some specific task. The winning solutions are so good at doing this specific task that they seem to show “intelligent” behaviour. However, as soon as they try to predict on a data point too far outside of what they were trained on, they fail: they show&nbsp;brittleness.</p>
<p>This is a problem plaguing current assessment of intelligence in LLMs. They focus strongly on how well these models perform in specific tasks, ignoring how intelligence is defined and measured in humans. And this is largely because, while researchers making these claims are very established in their fields, their fields are things like engineering, mathematics and computer sciences - not&nbsp;psychology.</p>
<p>Let’s have a look at an example that shows why these skill-based assessments are a problem. One of the biggest hype topics at the beginning of last year was people finding that ChatGPT and <span class="caps">GPT</span>-4 could solve <a href="https://www.businessinsider.com/chatgpt-passes-medical-exam-diagnoses-rare-condition-2023-4">medical</a> and <a href="https://www.cnet.com/tech/chatgpt-can-pass-the-bar-exam-does-that-actually-matter/">law</a> exams, with the resultant claims that it was going to replace lawyers and doctors. There were also examples of these models solving <a href="https://blog.cubed.run/can-chatgpt-beat-you-on-leetcode-b735ef8b45a5">leetcode puzzles</a>, with similar claims that programmers were going to be replaced. Let’s have a closer look at this last&nbsp;claim.</p>
<p>There was a <a href="https://x.com/cHHillee/status/1635790330854526981?t=bdVc5pxCn1P_GV3ZF91X6w&amp;s=35">great example</a> circulating on Twitter around the time <span class="caps">GPT</span>-4 came out. Horace He tested out <span class="caps">GPT</span>-4 with some coding challenges from a website called <a href="https://codeforces.com/">Codeforces</a>. What makes CodeForces really great for this little experiment is that the date that a particular problem was released is recorded. He collected 10 puzzles that were available when <span class="caps">GPT</span>-4 was trained, and ran them through the model, and what would you know, it got all of them&nbsp;right. </p>
<p><img src="/figure/codeforces-correct-answers.png" title="GPT-4 is coming for our jobs!" style="display: block; margin: auto;" /></p>
<p>He then collected another 10 puzzles that were released after <span class="caps">GPT</span>-4 was trained, of an equivalent level of difficulty - and this time <span class="caps">GPT</span>-4 got every single one of them wrong. What happened&nbsp;here?</p>
<p><img src="/figure/codeforces-incorrect-answers.png" title="Maybe we're safe for a while yet" style="display: block; margin: auto;" /></p>
<p>Essentially, all of the puzzles that <span class="caps">GPT</span>-4 solved were contained in the training set, and the model had simply memorized them. Sayash Kapoor <a href="https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks">tested this explicitly</a> by prompting the model to tell him about Codeforces puzzles that were available at the time it was trained, and <span class="caps">GPT</span>-4 obliging vomited up the complete source, confirming that it contained CodeForces puzzles in its training data. Testing your model on the data it was trained on is considered a <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets">fundamental error when measuring model performance</a>: of course your model will do well on a problem if it&#8217;s trained to solve that exact&nbsp;problem!</p>
<p><img src="/figure/sayash-kapoor-codeforces.png" title="Memorising the answers" style="display: block; margin: auto;" /></p>
<p>I think this is a very neat example that skill-based assessments of intelligence in LLMs can be wildly misleading. Instead, we need to look to how well these systems can solve tasks they’ve never seen: that is, how well they can&nbsp;generalise.</p>
<h2>Generalisation: a potential path to <span class="caps">AGI</span></h2>
<p>This idea of the generalisability of <span class="caps">AI</span> systems is explored deeply by François Chollet in <a href="https://arxiv.org/pdf/1911.01547">&#8220;On the Measure of Intelligence&#8221;</a>, where he defines a hierarchy of generalisation with five levels, going from least to most&nbsp;general.</p>
<p><strong>No generalisation</strong>: These are systems which do not generalise beyond the use case they were explicitly programmed to do, and includes systems which “know” all of the possible iterations or outcomes in advance. An example would be a program that uses all configurations of a tic-tac-toe game in order to play.<br>
<strong>Local generalisation</strong>: This is where a system can make inferences on examples it hasn’t seen before, but only if they are similar to the examples it was trained on. Machine learning models demonstrate local generalisation: when they try to make a prediction for an example that is too different to what they were trained on, they will fail, as we discussed earlier with Kaggle entries.<br>
<strong>Broad generalisation</strong>: This describes human-level ability in a single broad activity domain. A famous example of this is the <a href="https://medium.com/predict/the-turing-test-is-so-last-century-the-barista-test-for-artificial-general-intelligence-faf91034fa8c">Wozniak coffee test</a>, proposed by Steve Wozniak of Apple fame. He proposed that a system being able to go into a kitchen and make a cup of coffee without assistance would show broad generalisation. Full automated self-driving vehicles would also fall under this category.<br>
<strong>Extreme generalisation</strong>: This level is essentially human-level intelligence. This describes the ability of an artificial system to handle entirely new tasks that might only share abstract commonalities with previously encountered ones, and also apply these abilities across a wide scope of domains that humans might be reasonably expected to&nbsp;encounter.  </p>
<p>So you might be thinking: you’re focusing a lot on human-level skill. But we’re talking about an artificial system: don’t we want it to go beyond what we can do, to be better than us? And that leads to the final level of generalisation:<br>
<strong>Universality</strong>: This is the ability to handle any task within our universe, beyond the scope of tasks relevant to&nbsp;humans. </p>
<p>However, universality should be dismissed as at least an initial goal for artificial systems for a couple of reasons. Firstly, all systems need a scope to be useful, and the most immediate use case we have for <span class="caps">AI</span> models is to automate tasks normally done by people. Secondly, as you can see, we’re not even at the point of broad generalisation, so should focus on knocking off easier generalisation goals before we aim so&nbsp;high.</p>
<p>If we come back to intelligence, we can actually line up this idea of generalisation with the definition of human intelligence. The most widely accepted conceptualisation of human intelligence (the one that I learned during my PhD) is that humans have a general ability to learn, called general intelligence, or g. We use our general intelligence to learn broad activities, such as how to cook or drive a car, and within those broad abilities are the tasks that we can complete, such as whisking eggs or using an indicator. We can see an artificial system’s extreme generalisation aligns with g in humans, broad abilities align with broad generalisation, and task specific skills align with no generalisation or local&nbsp;generalisation.</p>
<p><img src="/figure/generalisation-and-human-intelligence.png" title="Aligning human and machine generalisation" style="display: block; margin: auto;" /></p>
<p>Given these similarities, it seems likely that we can derive lessons from measuring intelligence in humans, which is well-established field, and apply them to measuring intelligence in artificial systems. And as an aside, I’m aware that intelligence is a controversial area in psychology - however, the idea is that we can borrow ideas, like the ones above, which can help us create a definition that gets at the true generalisability of artificial&nbsp;systems.</p>
<h2>What might a system with <span class="caps">AGI</span> look&nbsp;like?</h2>
<p>In his paper, Chollet also gives us a blueprint for how we might be a system with artificial general&nbsp;intelligence. </p>
<p><img src="/figure/chollet-agi-system.png" title="A potential system with AGI" style="display: block; margin: auto;" /></p>
<p>He defines it as follows: an artificial system should demonstrate the ability to complete a task, using knowledge encoded in a skill program relevant to the task, which is generated by a human-like intelligent system. I hope you can see how these align to the general intelligence, broad abilities and task-specific abilities in humans that we just saw. These skill programs are refined based on input about
both the situation and the effectiveness of the response, and the intelligent system learns over time through exposure to more and more&nbsp;tasks.</p>
<p>Chollet also argues that if we’re focusing on the development of a human-like <span class="caps">AGI</span>, then we should also presume that such a system is designed to include the same innate skills that humans are born with: elementary geometry and physics, arithmetic, and the agency of other objects. Skill programs over time will be able to encode their experience with tasks and remember how they solved such problems before, and tasks can vary based on their generalisation difficulty. The generalisation difficulty of a task is how different the task is to previously encountered tasks - if the task varies greatly from things the system has been before, then the ability of the system to generalise must be greater to successfully complete the task. So obviously tasks high in generalisation difficulty are going to be the true test of such a&nbsp;system.</p>
<p>So what we have here is a starting point, a conceptualisation of how we might build a system with artificial general&nbsp;intelligence.</p>
<h2>Measuring <span class="caps">AGI</span></h2>
<p>So how might we assess how well a system can complete each task? Chollet defines it as&nbsp;follows:</p>
<div class="math">$$
\frac{1}{\text{All tasks in scope}} \cdot \left( \text{Value of achieving skill in task} \cdot \frac{1}{\text{All ways of solving task}} \cdot \left( \frac{\text{Generalisation difficulty}}{\text{Priors} + \text{Experience}} \right) \right)
$$</div>
<p>Don&#8217;t worry if this looks a bit hairy: we&#8217;re going to break it down. On the right hand side of the equation, we&nbsp;have:</p>
<div class="math">$$
\frac{\text{Generalisation difficulty}}{\text{Priors} + \text{Experience}}
$$</div>
<p>What this describes is the difficulty for this system of solving one specific task using one specific approach. We can think of the difficulty of solving this task using a specific approach as the generalisation difficulty of the task, divided by how much it aligns with the innate system priors and how much experience the system has with solving the task in this particular way. Solutions that are further away from what the system has seen or done before will be more&nbsp;“difficult”. </p>
<p>There are obviously different ways of solving the task, so we can average the difficulty of solving this task over all possible ways of solving it, to give the average difficulty of doing this task, hence we multiply the first term&nbsp;by: 
</p>
<div class="math">$$
\frac{1}{\text{All ways of solving task}}
$$</div>
<p>Finally, we can then add in a subjective component (<span class="math">\(\text{Value of achieving skill in task}\)</span>) which aims to capture how valuable or impressive it is that the system can do this task. It’s obviously much more impressive that a system can do brain surgery versus output a list of pseudorandom&nbsp;numbers.</p>
<p>We then average this across a range of tasks in the scope of what we would want an intelligent system to be able to do, so overall we have a measure of its skill-acquisition efficiency over a broad range of tasks, which we can think of as its generalisability or it&#8217;s artificial general intelligence. The real trick with building a working assessment using this system is defining a couple of quite tricky-to-define things, such as what tasks would need to be included in the scope to sufficiently measure generalisation, and how to assess the generalisation difficulty of tasks compared to what the system already&nbsp;knows.</p>
<p>Chollet went one step further and proposed a potential measurement for <span class="caps">AGI</span> based on his definition, called the <a href="https://lab42.global/arc/">Abstraction and Reasoning Corpus</a>. This consists of a series of 100 test puzzles, for which the taker is given a handful of examples, and is asked to determine the rules set out in these examples to solve the final example. This is thought to <a href="https://aiguide.substack.com/p/why-the-abstraction-and-reasoning">require the test taker to form internal abstractions to understand the rules being set out in each problem</a> - that is, to create a skill program to solve them. Here’s an example from the test: you can see for all three examples, that the idea is to change the colour of all squares contained in green squares from black to&nbsp;yellow.</p>
<p><img src="/figure/arc-example.png" title="ARC example" style="display: block; margin: auto;" /></p>
<p><span class="caps">ARC</span> is considered more robust than other ways of measuring intelligence in artificial systems for a couple of reasons. Firstly, the small number of examples means that a model can’t just brute force a puzzle by training on these. And secondly, the test set solutions have not been made publicly available, so they cannot be memorised by a model during training. While not a perfect tool, which Chollet will readily agree, it definitely goes beyond subjective assessments or tests designed for&nbsp;humans.</p>
<p>So, what does performance on <span class="caps">ARC</span> look like to date? Every year, a competition is held called the <a href="https://lab42.global/arcathon/"><span class="caps">ARC</span> Prize</a>, where the developers of the algorithm that can solve all 100 problems will receive prizes from a more than $1,000,000 pool. The <a href="https://lab42.global/past-challenges/2023-arcathon/">2023 winners</a> each only reached 30%, showing we still have quite a long way to <span class="caps">AGI</span> by this&nbsp;benchmark.</p>
<h2>How far are we away from <span class="caps">AGI</span>?</h2>
<p>Chollet’s work has been followed up by <a href="https://arxiv.org/pdf/2311.02462">researchers from DeepMind</a>. They again start from the idea that an intelligent artificial system must be able to generalise. However, they’ve simplified this dimension by breaking it into “narrow” and “general”&nbsp;systems.</p>
<p><img src="/figure/deepmind-levels-of-agi.png" title="Levels of AGI" style="display: block; margin: auto;" /></p>
<p>To this they add another dimension: performance, and propose that the way to measure this in an artificial system is the percent of people it can outperform. This ranges from no humans are outperformed, to people unskilled in that task, and goes up to 50%, 90%, 99%, and finally, all&nbsp;people.</p>
<p>Let’s first have a look at systems with narrow generalisation. In terms of systems that can outperform no one, they include have calculators and similar systems that need to be totally operated by humans. <a href="https://en.wikipedia.org/wiki/GOFAI"><span class="caps">GOFAI</span></a>, or good-old fashion <span class="caps">AI</span>, is classified as emerging narrow <span class="caps">AI</span> - the old school type <span class="caps">AI</span> based on hard-coded rules. At the higher ends, we have <a href="https://www.grammarly.com/">Grammarly</a>, which can outperform 90% of all people at grammar and spelling checking, our old friend Deep Blue, which can beat 99% of the population at chess, and <a href="https://alphafold.ebi.ac.uk/">AlphaFold</a>, which can predict a protein’s 3D structure better than 100% of humans, even skilled&nbsp;scientists.</p>
<p>But these are narrow systems. As we know, the true path to <span class="caps">AGI</span> is in systems that can generalise. On the general side, the DeepMind team put ChatGPT as emerging <span class="caps">AGI</span>, stating that it can outperform unskilled people across the full range of tasks you’d expect humans to be able to do. I disagree with this, as Chollet’s definition of the scope of generality is that it encompasses a wide range of tasks that humans are able to do, and even ChatGPT4o is quite limited in what it can do right now. However, perhaps they are talking about a future, ultra augmented version of ChatGPT, like the sort of system Andrej Karparthy discusses at the end of his <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">excellent introduction to LLMs</a>.</p>
<p>One final observation is that you can see that the rest of the general side of the scale is not filled yet, showing how much work these researchers think we have to go on the path of <span class="caps">AGI</span>. Coupled with Chollet’s definition and work with <span class="caps">ARC</span>, we can see how many steps we are away from creating an artificial system with <span class="caps">AGI</span>.</p>
<p>Like Chollet, the DeepMind researchers also discuss how we might assess our progress towards <span class="caps">AGI</span>. We have a number of benchmarks already, the most cutting edge being the <a href="https://github.com/google/BIG-bench"><span class="caps">BIG</span>-Bench</a>, which tries to assess new capabilities that LLMs may be developing, with tasks from predicting chess moves to guessing&nbsp;emojis. </p>
<p>However, let’s look beyond LLMs, which many argue are not a path to <span class="caps">AGI</span> at all, and think about assessments for <span class="caps">AI</span> systems more broadly. The DeepMind researchers argue that in order to reflect the way that humans solve problems, more unstructured, multistep problems may be needed, like Wozniak’s coffee test that we discussed&nbsp;earlier.</p>
<p>Finally, something that needs to be thought about is how we interact with this system. Unless we have a system that is totally autonomous, which may not be particularly desirable, we will need to have some way for humans to interact with this system. This means that perhaps this system should have some traits that humans require for successful social interactions, such as empathy and theory of mind, and such “social intelligence” traits should also be part of the benchmarking for <span class="caps">AGI</span>. The DeepMind team spend quite a lot of time defining these levels of autonomy in their paper, and it was a section I particularly enjoyed and found very&nbsp;thought-provoking.</p>
<p>With this, we&#8217;re at the end of our series, and I hope I have been able to convince you that claims of human-like abilities and behaviours in LLMs have been wildly overstated. LLMs, while impressive, are still just machine learning models with a range of strengths and weaknesses, and their illusions of humanity are just that - illusions. If we can demystify LLMs, we can instead focus on what they can do well, while also not being tempted to extend them past their capabilities. Using LLMs as intended - as powerful, but limited, natural language models - will allow us to get the most out of them right now without exposing ourselves to unnecessary&nbsp;risk.</p>
<p>If you liked this post, and you haven&#8217;t read the previous two in this series, you might be interested in my discussion of <a href="/blog/2024/06/can-llms-use-language-at-a-human-like-level.html">whether LLMs have human-level language use</a> or my <a href="/blog/2024/07/could-llms-be-sentient.html">exploration of whether LLMs could be sentient</a>.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

   </body>
</html>