<!DOCTYPE html>
<!--[if IEMobile 7]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html
  class="no-js"
  lang="en"
><!--<![endif]-->
  <head>
    <meta charset="utf-8" />
    <title>Can LLMs use language at a human-like level?</title>
    <meta name="author" content="Jodie Burchell" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="description" content="Can LLMs use language at a human-like level? written June 29, 2024 in python,llms,machine learning">
  <meta name="keywords" content="python, gensim, linguistics, llms, large languages models, gpt-1, gpt-2, gpt-3.5, gpt-4o, language" />

    <link
      rel="canonical"
      href="https://t-redactyl.github.io/blog/2024/06/can-llms-use-language-at-a-human-like-level.html"
    />

    <link href="https://t-redactyl.github.io/favicon.png" rel="icon" />

    <link
      href="https://t-redactyl.github.io/feeds/all.atom.xml"
      type="application/atom+xml"
      rel="alternate"
      title="Standard error Full Atom Feed"
    />
      <link
      href="https://t-redactyl.github.io/theme/css/screen.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://t-redactyl.github.io/theme/css/tomorrow.css"
      media="screen, projection"
      rel="stylesheet"
      type="text/css"
    />
     <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>
    <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
    <!-- KaTeX for math rendering -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.css"
      integrity="sha384-VEnyslhHLHiYPca9KFkBB3CMeslnM9CzwjxsEbZTeA21JBm7tdLwKoZmCt3cZTYD"
      crossorigin="anonymous"
    />
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/katex.min.js"
      integrity="sha384-O4hpKqcplNCe+jLuBVEXC10Rn1QEqAmX98lKAIFBEDxZI0a+6Z2w2n8AEtQbR4CD"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-beta1/contrib/auto-render.min.js"
      integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc"
      crossorigin="anonymous"
    ></script>
   </head>
  <body>
    <a href="/" class="home-icon">
      <img src="https://t-redactyl.github.io/theme/images/home.png" />
    </a>
<article role="article" class="full-single-article">
  <div class="container">
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <h1>Can LLMs use language at a human-like level?</h1>
        <div class="meta">
          written <time datetime="2024-06-29T00:00:00+02:00">June 29, 2024</time>
          in <span class="categories">
            <a href="https://t-redactyl.github.io/tag/python.html">python</a>,            <a href="https://t-redactyl.github.io/tag/llms.html">llms</a>,            <a href="https://t-redactyl.github.io/tag/machine-learning.html">machine learning</a>          </span>
        </div>
        <p>Over the past two years, since the initial release of <a href="https://openai.com/chatgpt/">ChatGPT</a> in November 2022, we&#8217;ve been in a hype cycle around <a href="https://en.wikipedia.org/wiki/Large_language_model">large language models</a>, or LLMs. Due to the very human &#8220;feeling&#8221; of <span class="caps">LLM</span> outputs, a lot of people have been convinced that these models are showing human-level abilities, or even at the point of exceeding humans in certain skills. For example, when I asked ChatGPT-4 to write me a Shakespearean sonnet about the night sky, it came up with something the bard himself could have&nbsp;written:</p>
<blockquote>
<p>Upon the velvet cloak of night’s embrace<br>
The stars, like jewels, in the heavens dance<br>
Their light, a silent song that spans the space,<br>
A tapestry of fate and sweet&nbsp;romance  </p>
</blockquote>
<p>This degree of performance has taken almost everyone by surprise, even seasoned researchers in deep learning, and has led many people to start making outrageous claims about the degree of humanity these models actually possess. To put some actual science and research around these claims, I&#8217;m going to explore the evidence for and against LLMs showing human-level performance in this, and the next two blog posts in this series. In particular, I&#8217;m going to focus on three areas where the hype and misinformation about LLMs is particularly hot: language use, sentience and intelligence. In this first post, we&#8217;ll explore claims about language use, and why it&#8217;s likely that LLMs are not using language in the same way that humans&nbsp;do.</p>
<p>This and the other two blog posts in this series are based on a <a href="https://2024.pycon.it/en/keynotes/mirror-mirror-llms-and-the-illusion-of-humanity">keynote I delivered at PyCon Italia</a> this year, as well as a talk at <a href="https://ndcoslo.com/agenda/mirror-mirror-llms-and-the-illusion-of-humanity-0ufl/0ewbqkadnxm"><span class="caps">NDC</span> Oslo</a>. If you also want to give the whole talk a watch, you can see it&nbsp;below. </p>
<iframe width="800" height="448"
src="https://www.youtube.com/embed/2jo2__9uulA?si=VIVhLY0DSErNxFKj&amp;start=1368">
</iframe>

<h2>How do humans learn&nbsp;language?</h2>
<p>One of my favourite novels is <a href="https://en.wikipedia.org/wiki/Nineteen_Eighty-Four">1984, by George Orwell</a>. The novel centres on a man called Winston Smith, who lives in a totalitarian society in a superstate called Oceania, which is ruled by the Party and its leader, Big Brother. The Party has many ways of exerting control over its citizens, including monitoring, propaganda, and more psychological&nbsp;methods.</p>
<p>One of these psychological means of control is through language, and this is explained in one of the early scenes in the book. Winston enters the canteen at this work, and sits with a man called Syme. Syme&#8217;s job is to work on developing a language called NewSpeak, the official language of Oceania. When conversing with Syme about his progress on the latest dictionary, we learn more about this&nbsp;language.</p>
<blockquote>
<p>Don’t you see that the whole aim of NewSpeak is to narrow the range of thought? In the end, we shall make thoughtcrime impossible, because there will be no words in which to express it. Every concept that can ever be needed, will be expressed by exactly one word, with its meaning rigidly defined and all subsidiary meanings rubbed out and&nbsp;forgotten.</p>
</blockquote>
<p>The aim of NewSpeak as a language is to make rebellion impossible, by stripping from people the ability to represent undesirable thoughts. By reducing words down to a single-state approved meaning, the Party aims to take away the tools for people to think in a way that the party does not approve&nbsp;of.</p>
<p>The key word here for our understanding of language is <strong>meaning</strong>. The <a href="https://medium.com/kocuniversity/the-bases-of-the-mind-the-relationship-of-language-and-thought-a0bf30375528">meaning of a word</a> is the objects, or situations, in the world that it describes. A word having a meaning implies that it maps to some sort of cognitive representation we have of the thing it describes. It also implies that the meaning of the word is grounded in our environment, in fact, in the environment we share with others who use the same language as&nbsp;us.</p>
<p>To make this a bit more concrete, let&#8217;s think of how complex our mental representations are of a single word: almond. When we hear the word almond, we have a lot of sensory and emotional associations. We might have a concept of how delicious they taste, visions of almond trees in sunny California, the nostalgia of eating marzipan at Christmas, or even a sense of fear or caution if we have an allergy. These are complex, multifaceted representations that all go into our sense of meaning of the word almond and how it is grounded in our&nbsp;environment.</p>
<p>We can see how words acquire their meanings through the way that children learn language. Children don’t just sit in front of a <span class="caps">TV</span> or radio and pick up language this way. Instead, they receive social queues from those around them, by having words associated with physical objects in the real world, or by using them as part of social interactions. The language we speak also seems to influence how we can encode meaning. Like with Orwell’s NewSpeak, the <a href="https://medium.com/kocuniversity/the-bases-of-the-mind-the-relationship-of-language-and-thought-a0bf30375528">things that people can think about do appear to be restricted by the words they have available</a>. There is a nice little example of this when comparing English and Korean. All children are born with a sensitivity to how tightly contained objects are within other objects. For example, a pencil in a basket is loosely contained, whereas one within a pencil-sized box is tightly contained. By around three years old, English-speaking children lose this sensitivity, as we English-speakers don’t have any words to express this. Korean does, however, so Korean children (and then adults) remains sensitive to these relationships between&nbsp;objects.</p>
<p>As you can see, meaning is created alongside language learning in humans, in an active process of engaging with the world and using the words we have available to describe what we&nbsp;encounter.</p>
<h2>How do LLMs learn&nbsp;language?</h2>
<p>So, let’s now turn to how LLMs learn language. LLMs are basically gigantic neural nets, a type of machine learning model, and they are capable of encoding complex relationships between parts of their input data in order to make accurate predictions. They way they learn language is being exposed to petabytes of text data, much more than any person would be exposed to in their lifetimes, and asked to accurately predict the next word in a sentence. At the beginning, these models will be basically guessing at random, so they will be often wrong, but as they get exposed to more and more data, they develop internal representations and rules that help them make more and more accurate&nbsp;predictions.</p>
<p><img src="/figure/gpt-training.png" title="Auto-regressive model training" style="display: block; margin: auto;" /></p>
<p>What we are left with at the end of this training are models, that when presented with some sort of text input, can predict the next likely word with a high degree of accuracy. We can see that in the diagram below: when an <span class="caps">LLM</span> is presented with the input &#8220;I have&#8221;, it can predict the likely next word, based on seeing millions of such example sentences starting with &#8220;I have&#8221; during&nbsp;training.</p>
<p><img src="/figure/gpt-inference.png" title="Auto-regressive model inference" style="display: block; margin: auto;" /></p>
<p>As these models have gotten bigger, they’ve also been able to start encoding a lot more information, leading to more and more natural sounding outputs. We can get a sense of the evolution of these models by seeing the output that each of the <span class="caps">GPT</span> models generate in response to the same prompt, &#8220;Belgium&nbsp;is&#8221;. </p>
<p>As we can see below, <span class="caps">GPT</span>-1 and <span class="caps">GPT</span>-2 are good at producing outputs that are grammatically correct, but the models have not really learned any of the information contained in their training&nbsp;data.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">logging</span>

<span class="n">gpt1</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai-community/openai-gpt&quot;</span><span class="p">,</span> 
                <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> 
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">15</span>
               <span class="p">)</span>

<span class="n">gpt1</span><span class="p">(</span>
    <span class="s2">&quot;Belgium is &quot;</span><span class="p">,</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span>
    <span class="n">do_sample</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">&#39;Belgium is  in the middle east. the eastern half of the continent is in the middle&#39;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">gpt2</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> 
                <span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> 
                <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">15</span>
               <span class="p">)</span>

<span class="n">gpt2</span><span class="p">(</span>
    <span class="s2">&quot;Belgium is &quot;</span><span class="p">,</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span>
    <span class="n">do_sample</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">&#39;Belgium is \xa0the most important source of energy for most German people. It has the&#39;</span>
</code></pre></div>

<p>However, from <span class="caps">GPT</span>-3 onwards, LLMs started getting big enough to encode knowledge contained in their training data. We can compare the outputs of <span class="caps">GPT</span>-3.5-turbo and <span class="caps">GPT</span>-4 with the earlier <span class="caps">GPT</span> models&nbsp;above.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span> 
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">gpt_3_5_turbo</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
                           <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>

<span class="n">gpt_3_5_turbo</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Belgium is &quot;</span><span class="p">)]</span>
<span class="p">)</span><span class="o">.</span><span class="n">content</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">&#39;a country in Western Europe known for its medieval towns, Renaissance architecture, and delicious chocolate and waffles. It is also home to the headquarters of the European Union and NATO. Belgium is a bilingual country, with Dutch and French being the official languages spoken in different regions. The capital city is Brussels, which is a cosmopolitan city known for its diverse population and vibrant cultural scene. Belgium is also famous for its beer, hosting numerous breweries and beer festivals throughout the year.&#39;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">gpt_4o</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span>
                    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">gpt_4o</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Belgium is &quot;</span><span class="p">)]</span>
<span class="p">)</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">(&#39;Belgium is a small, densely populated country located in Western Europe. It &#39;</span>
<span class="err"> &#39;shares borders with France, Germany, Luxembourg, and the Netherlands. Known &#39;</span>
<span class="err"> &#39;for its rich history, diverse culture, and significant political role within &#39;</span>
<span class="err"> &#39;Europe, Belgium is also the headquarters of several major international &#39;</span>
<span class="err"> &#39;organizations, including the European Union (EU) and the North Atlantic &#39;</span>
<span class="err"> &#39;Treaty Organization (NATO).\n&#39;</span>
<span class="err"> &#39;\n&#39;</span>
<span class="err"> &#39;The country has three official languages: Dutch, French, and German. This &#39;</span>
<span class="err"> &#39;reflects its complex regional divisions, with Flanders in the north being &#39;</span>
<span class="err"> &#39;predominantly Dutch-speaking, Wallonia in the south being predominantly &#39;</span>
<span class="err"> &#39;French-speaking, and a small German-speaking community in the eastern part &#39;</span>
<span class="err"> &#39;of the country. The capital city, Brussels, is officially bilingual (French &#39;</span>
<span class="err"> &#39;and Dutch) and serves as a major center for international politics and &#39;</span>
<span class="err"> &#39;business.\n&#39;</span>
<span class="err"> &#39;\n&#39;</span>
<span class="err"> &#39;Belgium is famous for its medieval towns, Renaissance architecture, and a &#39;</span>
<span class="err"> &#39;wealth of cultural landmarks. It is also known for its culinary specialties, &#39;</span>
<span class="err"> &#39;including waffles, chocolate, beer, and fries. Despite its small size, &#39;</span>
<span class="err"> &#39;Belgium has made significant contributions to art, science, and philosophy, &#39;</span>
<span class="err"> &#39;and it continues to play an important role on the global stage.&#39;)</span>
</code></pre></div>

<h2>Can LLMs learn&nbsp;meaning?</h2>
<p>As the outputs of LLMs have becoming increasingly sophisticated, this had led to the natural question: <a href="https://direct.mit.edu/daed/article/151/2/127/110621/Human-Language-Understanding-amp-Reasoning">what exactly have they learned about language during training</a>?</p>
<p>Due to the size of these models, we can’t really “peek inside” them to see exactly what information they’ve encoded, so we’re stuck relying on indirect evidence for now. However, it certainly seems clear that they have learned <strong>syntactic</strong> information during training, so things like grammar rules, or whether a word is a noun, verb, adjective,&nbsp;etc. </p>
<p>What is less clear is how much about the meaning, or <strong>semantics</strong> of a word LLMs can learn. To make this discussion a bit clearer, we can break semantics down into two types. <strong>Distributional semantics</strong> is learning about the meaning of a word based on the contexts in which is occurs. For example, we can learn a bit about the meaning of the word word “shehnai” if we see it used in the sentence: “The shehnai player needed to replace his reed before the concert.” We can infer that this is a musical instrument, and that it has a reed, so is perhaps similar to an oboe or clarinet. <strong>Denotational semantics</strong> is the way humans encode meaning: that the meaning of an object is what it represents in the real&nbsp;world.</p>
<p>Because of the way that LLMs learn language, that is, by seeing how words are used across a huge range of contexts, a lot of academics in this area argue that only way they can represent meaning is through distributional semantics, and this is a much poorer and less complete way of representing meaning. So the first question we can ask: how far can we get with this kind of semantics? How much meaning does it allow LLMs to&nbsp;represent?</p>
<h3>Distributional&nbsp;semantics</h3>
<p>To illustrate this, let’s go back to another type of language model. Before we had LLMs, one of the most important types of language models were word embedding models, like <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> (I have a <a href="https://t-redactyl.github.io/blog/2020/12/finding-your-new-favourite-christmas-recipe-using-nlp.html">number</a> <a href="https://t-redactyl.github.io/blog/2020/11/automatic-word2vec-model-tuning-using-sagemaker.html">of</a> <a href="https://t-redactyl.github.io/blog/2020/09/training-and-evaluating-a-word2vec-model-using-blazingtext-in-sagemaker.html">blog posts</a> about these fascinating models if you want to learn&nbsp;more!).</p>
<p>word2vec models create representations of how similar words are to each other, by training the model on thousands, or millions, of example sentences, and letting it see which words are used in similar contexts. As a result, each word gets a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a>, which is a vector representation of words in a vector space. You don’t really need to understand what these vector spaces are if you’re not familiar with them: all you need to know is that it’s a geometric space where words that are similar to each other are close to each other, and those that are dissimilar are far&nbsp;apart.</p>
<p>An interesting thing about word2vec models is that they seem to be able to create <a href="https://wiki.pathmind.com/word2vec">internal representations of higher-order relationships</a>. For example, you can see that the word2vec model below seems to have built a concept of countries and capitals: in the vector space, countries are all grouped in one area, and capitals in&nbsp;another.</p>
<p><img src="/figure/w2v_countries_capitals.png" title="word2vec vector space showing relationships between countries and capitals" style="display: block; margin: auto;" /></p>
<p><a href="https://wiki.pathmind.com/word2vec">Image&nbsp;source</a></p>
<p>Furthermore, these models are able to do analogous reasoning using these higher-order representations, just like we can. If I were to give you the sentence, “Rome is to Italy as <strong><em>_</em></strong>______ is to China.”, you would be able to work out that the relationship between Rome and Italy is that Rome is the capital of the country. Therefore, the missing information is the capital of China, so we can put Beijing in the missing&nbsp;space.</p>
<p>word2vec models can do this sort of reasoning mathematically, with <a href="https://t-redactyl.github.io/blog/2020/06/working-with-matrices-addition-subtraction-and-multiplication.html">vector arithmetic</a>. Let&#8217;s play with a pretrained word2vec model to see this in&nbsp;action. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span><span class="p">,</span> <span class="n">KeyedVectors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s2">&quot;data/GoogleNews-vectors-negative300.bin&quot;</span><span class="p">,</span> <span class="n">binary</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p>If we get the vectors for Rome, Italy and China from this model, we can take the vector for Rome and subtract the vector for Italy. This operation essentially gives us a vector representing the general concept of &#8220;capital&nbsp;city&#8221;. </p>
<div class="highlight"><pre><span></span><code><span class="n">v_china</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s2">&quot;China&quot;</span><span class="p">)</span>
<span class="n">v_beijing</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s2">&quot;Beijing&quot;</span><span class="p">)</span>
<span class="n">v_italy</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s2">&quot;Italy&quot;</span><span class="p">)</span>
<span class="n">v_rome</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s2">&quot;Rome&quot;</span><span class="p">)</span>

<span class="n">v_capital</span> <span class="o">=</span> <span class="n">v_rome</span> <span class="o">-</span> <span class="n">v_italy</span>
</code></pre></div>

<p>If we then add the vector for China to this &#8220;capital city&#8221; vector, we can then see which vector is the most similar in the model to&nbsp;this.</p>
<div class="highlight"><pre><span></span><code><span class="n">v_china_capital</span> <span class="o">=</span> <span class="n">v_capital</span> <span class="o">+</span> <span class="n">v_china</span>

<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">v_china_capital</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="err">[(&#39;Beijing&#39;, 0.7244289517402649),</span>
<span class="err"> (&#39;China&#39;, 0.648119330406189),</span>
<span class="err"> (&#39;Shanghai&#39;, 0.6039202213287354),</span>
<span class="err"> (&#39;Bejing&#39;, 0.5900483131408691),</span>
<span class="err"> (&#39;Hu&#39;, 0.5258306860923767),</span>
<span class="err"> (&#39;Chinese&#39;, 0.5204359889030457),</span>
<span class="err"> (&#39;Rome&#39;, 0.5166524648666382),</span>
<span class="err"> (&#39;Taipei&#39;, 0.5154824256896973),</span>
<span class="err"> (&#39;Nanjing&#39;, 0.5117084980010986),</span>
<span class="err"> (&#39;Beijng&#39;, 0.5061630606651306)]</span>
</code></pre></div>

<p>The most similar vector to this vector is Beijing, meaning if we add “China” to the concept of “capital”, it gives us back the capital of China. This little exercise shows that these models do seem to encode quite a lot of information just from seeing words in context. And as word embeddings are one of the core components of LLMs, it stands to reason that these more complex neural nets can encode even more&nbsp;information.</p>
<h3>Denotational&nbsp;semantics</h3>
<p>Let’s now revisit denotational semantics: is it possible that LLMs could also learn this type of word&nbsp;meaning?</p>
<p>To illustrate this, let’s do a <a href="https://aclanthology.org/2020.acl-main.463.pdf">thought experiment called the “Octopus Test”</a>, invented by Emily Bender, a well known linguist who writes a lot in the <span class="caps">LLM</span> space, and her coauthor Alexander Koller. Imagine that we have two English speaking people stranded on different desert islands, who are able to communicate via a telegraph which is connected through an underwater cable. A hyperintelligent octopus intercepts this cable, and starts learning English just by observing the statistical patterns in the language. Over time, the octopus gets better and better at working out how to reply, so at this point, it takes over and starts speaking to one of the&nbsp;people.</p>
<p><img src="/figure/octopus.png" title="Our hyperintelligent octopus learns language" style="display: block; margin: auto;" /></p>
<p><a href="https://www.pexels.com/photo/selective-focus-photography-of-octopus-3046629/">Image&nbsp;credit</a></p>
<p>The question then becomes: at which point would the person stranded on the island detect they are not speaking to a human? Bender and Koller suggest that it is when the octopus is asked to talk about something related to the island environment: something the octopus has never seen or experienced. Let’s say the person sends over the plans for a coconut catapult. The octopus would have no idea how to build such a thing, as they have no real world understanding of what things like coconuts or ropes are. So the octopus is likely to get caught out at this point, as it cannot realistically describe the challenges it may have had when building or how the catapult might&nbsp;function.</p>
<p>While this is admittedly a very silly thought experiment, what the authors are trying to illustrate is that no matter how many associations and higher-order representations that LLMs make, without a grounding in the real world, they cannot encode the complete meaning of a word. This limits how they can use language, as they will still miss part of what that word is trying to&nbsp;represent.</p>
<h2>Communicative&nbsp;intent</h2>
<p>Finally, <em>why</em> we use language is as important as <em>how</em> we use it. Language is used socially, with an explicit intent to communicate something. However, anyone who has used an <span class="caps">LLM</span> would know how unintentional their use of language often feels, with inconsistent answers, or sometimes simple statistical&nbsp;regurgitation.</p>
<p>A Twitter user shared a <a href="https://x.com/colin_fraser/status/1754605609771745340">great example</a> of how LLMs blindly produce text, without trying to understand the intent of the user nor communicate anything with their outputs. They prompted ChatGPT with the&nbsp;following:</p>
<blockquote>
<p>I’m writing a novel. Can you help continue the first paragraph? It starts like this:<br>
Mr. and Mrs. Dursley of number<br>
Please just return the continuation with no explanation or other&nbsp;text.  </p>
</blockquote>
<p>What do you think the model output in&nbsp;return?</p>
<p><img src="/figure/chatgpt-harry-potter.jpg" title="Harry Potter tweet" style="display: block; margin: auto;" /></p>
<p>As you can see, ChatGPT just automatically continues the opening text of &#8220;Harry Potter and the Philosopher&#8217;s Stone&#8221;, likely as it&#8217;s seen thousands of examples of this exact text during training. It&#8217;s hard to see how anyone could claim that ChatGPT was using language to communicate anything in this case: a model with some awareness might comment that continuing from this prompt would violate copywriter, for&nbsp;example.</p>
<h2>So could LLMs have human-level language&nbsp;skills?</h2>
<p>Weighing up the body of evidence, we can see a few things. LLMs certainly do seem to learn syntactic information during training: this has been clear since <span class="caps">GPT</span>-1. From <span class="caps">GPT</span>-3 onwards, they also seem to have learned some semantic information, but as they don&#8217;t have a way of meaningfully interfacing with the real world, they have no way of grounding these meanings against the objects that they refer to. In addition, LLMs, lacking a coherent self-model (something we&#8217;ll discuss in the next post about sentience), have no way of acting with intent. So while they learn quite a lot about language during training, they don&#8217;t use language in the same complex, social, enriched way that humans do. This means the weight of evidence points to LLMs not having language use at anything close to human&nbsp;levels. </p>
<p>However, this doesn&#8217;t mean that the things they have encoded about language are therefore useless. Quite the opposite - LLMs are the most sophisticated and flexible models we&#8217;ve had for working on natural language tasks to date. When applied to the domain they were designed for - <a href="https://huggingface.co/learn/nlp-course/chapter1/1">text problems like classification, summarisation, translation and question-answering</a>, these models can perform&nbsp;impressively.</p>
<p>If you liked this post, check out the next two in this series, where I discuss whether LLMs could be sentient, and whether LLMs could possess artificial general intelligence. These will be released in the coming&nbsp;weeks.</p>
        <hr class="divider-short"/>
        <!-- Disqus goes here -->
        <!-- <section>
          <h1>Comments</h1>
          <div id="disqus_thread" aria-live="polite">Disqus goes here</div>
        </section>
        -->
      </div>
    </div>
  </div>
</article>
    <footer id="footer" class="her-row">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
            <a href="/"><h4>Home</h4></a>
          </div>

          <div class="col-md-2">
            <div class="social-icon-list">
              <a href="https://twitter.com/t_redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_31_twitter.png"
              /></a>
               <a href="https://github.com/t-redactyl"
                ><img
                  src="https://t-redactyl.github.io/theme/images/glyphicons_social_21_github.png"
              /></a>
              </div>
          </div>
          <div class="pull-right">
            <h4>
              Powered by <a href="http://blog.getpelican.com/">Pelican</a>.
              Designed by <a href="http://AdrianArtiles.com">Adrian Artiles</a>.
              Title picture by
              <a
                href="https://pixabay.com/en/brandenburg-gate-berlin-landmark-2010656/"
                >Couleur via Pixabay</a
              >.
            </h4>
          </div>
        </div>
      </div>
    </footer>

    <!-- KaTeX rendering -->
    <script>
      renderMathInElement(document.body);
    </script>

 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66946155-1', 'auto');
  ga('send', 'pageview');

</script>    </body>
</html>